{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, balanced_accuracy_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "      LITAF      TPBG     CCRL2    ZNF268     CCT6B      CD163     CD320  \\\n0  7.108456  7.463690  3.841445  4.848988  2.944146   5.316088  6.127067   \n1  8.271182  9.319451  4.015521  3.670656  2.949599   7.182012  5.874283   \n2  7.894494  8.052706  5.259943  5.552700  3.145896   7.906607  4.067954   \n3  8.651722  9.158700  3.778845  5.629598  3.096840   7.099068  5.485791   \n4  8.440454  6.642578  5.889477  4.645318  3.188979  10.478267  5.705050   \n\n      KIF17     KIF1A      CD44  ...     WDR62    ADORA3     AP1B1       HGD  \\\n0  3.629676  6.434729  6.424752  ...  3.472400  3.884389  6.602958  2.997926   \n1  3.501298  2.533096  7.274651  ...  3.302293  4.162791  6.752108  4.526801   \n2  3.641523  3.108423  8.087042  ...  2.748002  5.146482  7.613484  2.615693   \n3  3.600242  3.262309  6.773035  ...  3.646378  3.597345  6.584821  5.801371   \n4  3.607868  3.293778  6.724959  ...  4.108428  7.332183  8.042622  3.921174   \n\n    WHSC1L1     ADRB3  ADAMTSL2     ANXA7      ABI1       AHR  \n0  6.027037  3.148324  3.284457  8.879693  7.774435  8.399100  \n1  2.932985  3.351544  3.711498  8.241469  6.846877  7.129267  \n2  3.983403  3.288953  3.839644  8.194948  8.402202  8.111880  \n3  3.740626  3.062545  3.242531  8.907759  7.684565  6.964725  \n4  3.752512  3.593116  3.241774  8.678762  7.637010  6.882944  \n\n[5 rows x 500 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LITAF</th>\n      <th>TPBG</th>\n      <th>CCRL2</th>\n      <th>ZNF268</th>\n      <th>CCT6B</th>\n      <th>CD163</th>\n      <th>CD320</th>\n      <th>KIF17</th>\n      <th>KIF1A</th>\n      <th>CD44</th>\n      <th>...</th>\n      <th>WDR62</th>\n      <th>ADORA3</th>\n      <th>AP1B1</th>\n      <th>HGD</th>\n      <th>WHSC1L1</th>\n      <th>ADRB3</th>\n      <th>ADAMTSL2</th>\n      <th>ANXA7</th>\n      <th>ABI1</th>\n      <th>AHR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.108456</td>\n      <td>7.463690</td>\n      <td>3.841445</td>\n      <td>4.848988</td>\n      <td>2.944146</td>\n      <td>5.316088</td>\n      <td>6.127067</td>\n      <td>3.629676</td>\n      <td>6.434729</td>\n      <td>6.424752</td>\n      <td>...</td>\n      <td>3.472400</td>\n      <td>3.884389</td>\n      <td>6.602958</td>\n      <td>2.997926</td>\n      <td>6.027037</td>\n      <td>3.148324</td>\n      <td>3.284457</td>\n      <td>8.879693</td>\n      <td>7.774435</td>\n      <td>8.399100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.271182</td>\n      <td>9.319451</td>\n      <td>4.015521</td>\n      <td>3.670656</td>\n      <td>2.949599</td>\n      <td>7.182012</td>\n      <td>5.874283</td>\n      <td>3.501298</td>\n      <td>2.533096</td>\n      <td>7.274651</td>\n      <td>...</td>\n      <td>3.302293</td>\n      <td>4.162791</td>\n      <td>6.752108</td>\n      <td>4.526801</td>\n      <td>2.932985</td>\n      <td>3.351544</td>\n      <td>3.711498</td>\n      <td>8.241469</td>\n      <td>6.846877</td>\n      <td>7.129267</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.894494</td>\n      <td>8.052706</td>\n      <td>5.259943</td>\n      <td>5.552700</td>\n      <td>3.145896</td>\n      <td>7.906607</td>\n      <td>4.067954</td>\n      <td>3.641523</td>\n      <td>3.108423</td>\n      <td>8.087042</td>\n      <td>...</td>\n      <td>2.748002</td>\n      <td>5.146482</td>\n      <td>7.613484</td>\n      <td>2.615693</td>\n      <td>3.983403</td>\n      <td>3.288953</td>\n      <td>3.839644</td>\n      <td>8.194948</td>\n      <td>8.402202</td>\n      <td>8.111880</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.651722</td>\n      <td>9.158700</td>\n      <td>3.778845</td>\n      <td>5.629598</td>\n      <td>3.096840</td>\n      <td>7.099068</td>\n      <td>5.485791</td>\n      <td>3.600242</td>\n      <td>3.262309</td>\n      <td>6.773035</td>\n      <td>...</td>\n      <td>3.646378</td>\n      <td>3.597345</td>\n      <td>6.584821</td>\n      <td>5.801371</td>\n      <td>3.740626</td>\n      <td>3.062545</td>\n      <td>3.242531</td>\n      <td>8.907759</td>\n      <td>7.684565</td>\n      <td>6.964725</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.440454</td>\n      <td>6.642578</td>\n      <td>5.889477</td>\n      <td>4.645318</td>\n      <td>3.188979</td>\n      <td>10.478267</td>\n      <td>5.705050</td>\n      <td>3.607868</td>\n      <td>3.293778</td>\n      <td>6.724959</td>\n      <td>...</td>\n      <td>4.108428</td>\n      <td>7.332183</td>\n      <td>8.042622</td>\n      <td>3.921174</td>\n      <td>3.752512</td>\n      <td>3.593116</td>\n      <td>3.241774</td>\n      <td>8.678762</td>\n      <td>7.637010</td>\n      <td>6.882944</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 500 columns</p>\n</div>"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "X_train, y_train = train_df[train_df.columns.difference([\"patient_ID\", \"posOutcome\"])], train_df[\"posOutcome\"]\n",
    "\n",
    "ft_500 = []\n",
    "with open(\"datasets/xgb500_genes.txt\", \"r\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        ft_500.append(line.strip())\n",
    "\n",
    "X_train_500 = X_train[ft_500]\n",
    "X_train_500.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "params = {'n_estimators': [300, 400, 500, 600, 700],\n",
    "              'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07],\n",
    "              'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "              'max_depth': [3, 4, 5, 6],\n",
    "              'subsample': [0.6, 0.8, 1.0],\n",
    "              'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "              'min_child_weight': [1, 2, 3, 4, 5]}\n",
    "\n",
    "init_points = {\n",
    "              'max_depth': [3, 8, 3, 8, 8, 3, 8, 3],\n",
    "              'gamma':    [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "              'min_child_weight': [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "              'subsample':  [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'colsample_bytree': [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'learning_rate': [0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.02],\n",
    "              'n_   estimators': [400, 600, 400, 600, 400, 600, 700, 500]\n",
    "              }\n",
    "\n",
    "pbounds = {\n",
    "             'n_estimators': (100, 800),\n",
    "             'learning_rate': (0.01, 0.09),\n",
    "             'max_depth': (2, 12),\n",
    "             'gamma': (0.001, 10.0),\n",
    "             'min_child_weight': (0, 20),\n",
    "             'subsample': (0.4, 1.0),\n",
    "             'colsample_bytree' :(0.4, 1.0)\n",
    "            }\n",
    "\n",
    "def bayesian_opt(train, target, n_jobs=-1):\n",
    "    def xgb_opt(n_estimators, learning_rate, gamma, max_depth,\n",
    "                subsample, colsample_bytree, min_child_weight):\n",
    "\n",
    "        received_params = {\n",
    "            'n_estimators': int(n_estimators),\n",
    "            'learning_rate': learning_rate,\n",
    "            'gamma': gamma,\n",
    "            'max_depth': int(max_depth),\n",
    "            'subsample': max(min(subsample, 1), 0),\n",
    "            'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'random_state': rand_seed,\n",
    "            'objective' : 'binary:logistic'\n",
    "        }\n",
    "        st_cv = StratifiedKFold(shuffle=True, random_state=rand_seed)\n",
    "        clf = XGBClassifier(**received_params, n_jobs=4)\n",
    "        cv_results = cross_validate(clf, train, target, cv=st_cv,\n",
    "                                    n_jobs=14, scoring='roc_auc',\n",
    "                                    return_train_score=True)\n",
    "        val_score = np.mean(cv_results[\"test_score\"])\n",
    "        train_score = np.mean(cv_results[\"train_score\"])\n",
    "        print(\"Mean test auc: {0:.2%},  Mean train auc: {1:.2%}, Diff: {2:.2%}\".format(\n",
    "              val_score, train_score, (train_score-val_score)))\n",
    "\n",
    "        return (val_score * 2) - 1\n",
    "\n",
    "    return xgb_opt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Mean test auc: 83.73%,  Mean train auc: 99.13%, Diff: 15.40%\n",
      "| \u001B[0m 1       \u001B[0m | \u001B[0m 0.6746  \u001B[0m | \u001B[0m 0.6247  \u001B[0m | \u001B[0m 9.507   \u001B[0m | \u001B[0m 0.06856 \u001B[0m | \u001B[0m 7.987   \u001B[0m | \u001B[0m 3.12    \u001B[0m | \u001B[0m 209.2   \u001B[0m | \u001B[0m 0.4349  \u001B[0m |\n",
      "Mean test auc: 84.30%,  Mean train auc: 99.91%, Diff: 15.61%\n",
      "| \u001B[95m 2       \u001B[0m | \u001B[95m 0.686   \u001B[0m | \u001B[95m 0.9197  \u001B[0m | \u001B[95m 6.012   \u001B[0m | \u001B[95m 0.06665 \u001B[0m | \u001B[95m 2.206   \u001B[0m | \u001B[95m 19.4    \u001B[0m | \u001B[95m 682.7   \u001B[0m | \u001B[95m 0.5274  \u001B[0m |\n",
      "Mean test auc: 85.08%,  Mean train auc: 100.00%, Diff: 14.92%\n",
      "| \u001B[95m 3       \u001B[0m | \u001B[95m 0.7016  \u001B[0m | \u001B[95m 0.5091  \u001B[0m | \u001B[95m 1.835   \u001B[0m | \u001B[95m 0.03434 \u001B[0m | \u001B[95m 7.248   \u001B[0m | \u001B[95m 8.639   \u001B[0m | \u001B[95m 303.9   \u001B[0m | \u001B[95m 0.7671  \u001B[0m |\n",
      "Mean test auc: 84.71%,  Mean train auc: 99.97%, Diff: 15.26%\n",
      "| \u001B[0m 4       \u001B[0m | \u001B[0m 0.6943  \u001B[0m | \u001B[0m 0.4837  \u001B[0m | \u001B[0m 2.922   \u001B[0m | \u001B[0m 0.03931 \u001B[0m | \u001B[0m 6.561   \u001B[0m | \u001B[0m 15.7    \u001B[0m | \u001B[0m 239.8   \u001B[0m | \u001B[0m 0.7085  \u001B[0m |\n",
      "Mean test auc: 85.77%,  Mean train auc: 100.00%, Diff: 14.23%\n",
      "| \u001B[95m 5       \u001B[0m | \u001B[95m 0.7154  \u001B[0m | \u001B[95m 0.4617  \u001B[0m | \u001B[95m 1.162   \u001B[0m | \u001B[95m 0.05225 \u001B[0m | \u001B[95m 8.201   \u001B[0m | \u001B[95m 8.075   \u001B[0m | \u001B[95m 302.1   \u001B[0m | \u001B[95m 0.8856  \u001B[0m |\n",
      "Mean test auc: 85.55%,  Mean train auc: 100.00%, Diff: 14.45%\n",
      "| \u001B[0m 6       \u001B[0m | \u001B[0m 0.7109  \u001B[0m | \u001B[0m 0.4867  \u001B[0m | \u001B[0m 2.975   \u001B[0m | \u001B[0m 0.02633 \u001B[0m | \u001B[0m 7.658   \u001B[0m | \u001B[0m 8.173   \u001B[0m | \u001B[0m 301.0   \u001B[0m | \u001B[0m 0.8608  \u001B[0m |\n",
      "Mean test auc: 84.89%,  Mean train auc: 99.98%, Diff: 15.09%\n",
      "| \u001B[0m 7       \u001B[0m | \u001B[0m 0.6978  \u001B[0m | \u001B[0m 0.4579  \u001B[0m | \u001B[0m 2.321   \u001B[0m | \u001B[0m 0.02116 \u001B[0m | \u001B[0m 10.28   \u001B[0m | \u001B[0m 7.452   \u001B[0m | \u001B[0m 301.6   \u001B[0m | \u001B[0m 0.531   \u001B[0m |\n",
      "Mean test auc: 85.08%,  Mean train auc: 99.96%, Diff: 14.88%\n",
      "| \u001B[0m 8       \u001B[0m | \u001B[0m 0.7017  \u001B[0m | \u001B[0m 0.4649  \u001B[0m | \u001B[0m 0.124   \u001B[0m | \u001B[0m 0.02744 \u001B[0m | \u001B[0m 10.1    \u001B[0m | \u001B[0m 10.37   \u001B[0m | \u001B[0m 300.3   \u001B[0m | \u001B[0m 0.5266  \u001B[0m |\n",
      "Mean test auc: 84.55%,  Mean train auc: 100.00%, Diff: 15.45%\n",
      "| \u001B[0m 9       \u001B[0m | \u001B[0m 0.691   \u001B[0m | \u001B[0m 0.6222  \u001B[0m | \u001B[0m 3.409   \u001B[0m | \u001B[0m 0.0759  \u001B[0m | \u001B[0m 5.333   \u001B[0m | \u001B[0m 9.694   \u001B[0m | \u001B[0m 300.8   \u001B[0m | \u001B[0m 0.5885  \u001B[0m |\n",
      "Mean test auc: 84.71%,  Mean train auc: 99.48%, Diff: 14.77%\n",
      "| \u001B[0m 10      \u001B[0m | \u001B[0m 0.6942  \u001B[0m | \u001B[0m 0.9542  \u001B[0m | \u001B[0m 2.144   \u001B[0m | \u001B[0m 0.01514 \u001B[0m | \u001B[0m 8.597   \u001B[0m | \u001B[0m 9.694   \u001B[0m | \u001B[0m 302.2   \u001B[0m | \u001B[0m 0.523   \u001B[0m |\n",
      "Mean test auc: 84.69%,  Mean train auc: 100.00%, Diff: 15.31%\n",
      "| \u001B[0m 11      \u001B[0m | \u001B[0m 0.6938  \u001B[0m | \u001B[0m 0.7313  \u001B[0m | \u001B[0m 0.9127  \u001B[0m | \u001B[0m 0.01346 \u001B[0m | \u001B[0m 5.371   \u001B[0m | \u001B[0m 6.395   \u001B[0m | \u001B[0m 301.8   \u001B[0m | \u001B[0m 0.9832  \u001B[0m |\n",
      "Mean test auc: 84.92%,  Mean train auc: 100.00%, Diff: 15.08%\n",
      "| \u001B[0m 12      \u001B[0m | \u001B[0m 0.6984  \u001B[0m | \u001B[0m 0.5105  \u001B[0m | \u001B[0m 0.3643  \u001B[0m | \u001B[0m 0.0757  \u001B[0m | \u001B[0m 8.397   \u001B[0m | \u001B[0m 8.784   \u001B[0m | \u001B[0m 301.3   \u001B[0m | \u001B[0m 0.8608  \u001B[0m |\n",
      "Mean test auc: 85.08%,  Mean train auc: 100.00%, Diff: 14.92%\n",
      "| \u001B[0m 13      \u001B[0m | \u001B[0m 0.7016  \u001B[0m | \u001B[0m 0.7884  \u001B[0m | \u001B[0m 2.331   \u001B[0m | \u001B[0m 0.02652 \u001B[0m | \u001B[0m 7.756   \u001B[0m | \u001B[0m 7.834   \u001B[0m | \u001B[0m 300.9   \u001B[0m | \u001B[0m 0.6758  \u001B[0m |\n",
      "Mean test auc: 85.08%,  Mean train auc: 100.00%, Diff: 14.92%\n",
      "| \u001B[0m 14      \u001B[0m | \u001B[0m 0.7015  \u001B[0m | \u001B[0m 0.7653  \u001B[0m | \u001B[0m 3.284   \u001B[0m | \u001B[0m 0.03851 \u001B[0m | \u001B[0m 7.444   \u001B[0m | \u001B[0m 8.29    \u001B[0m | \u001B[0m 299.1   \u001B[0m | \u001B[0m 0.4349  \u001B[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_500_bo = bayesian_opt(X_train, y_train)\n",
    "xgb_bo_500 = BayesianOptimization(f=xgb_500_bo, pbounds=pbounds,\n",
    "                              random_state=rand_seed,\n",
    "                              verbose=2,)\n",
    "# xgb_bo_500.probe(params=init_points, lazy=True)\n",
    "xgb_bo_500.maximize(init_points=4, n_iter=10, acq='ei', xi=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8096969696969694\n",
      "Params: {'colsample_bytree': 0.9991337809826765, 'gamma': 0.5126040322793047, 'learning_rate': 0.07260519432257369, 'max_depth': 9.423245502822606, 'min_child_weight': 15.847405956328881, 'n_estimators': 344.4967382859727, 'subsample': 0.6786490570865282}\n",
      "Iteration 0: \n",
      "\t{'target': 0.7634889434889438, 'params': {'colsample_bytree': 0.6247240713084175, 'gamma': 9.507192349792751, 'learning_rate': 0.0685595153449124, 'max_depth': 7.986584841970366, 'min_child_weight': 3.1203728088487304, 'n_estimators': 209.19616423534185, 'subsample': 0.4348501673009197}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.7854873054873057, 'params': {'colsample_bytree': 0.9197056874649611, 'gamma': 6.011549002420345, 'learning_rate': 0.06664580622368364, 'max_depth': 2.2058449429580245, 'min_child_weight': 19.398197043239886, 'n_estimators': 682.7098485602952, 'subsample': 0.5274034664069657}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.7578542178542178, 'params': {'colsample_bytree': 0.8056204439897416, 'gamma': 6.3870021075196295, 'learning_rate': 0.07760138007734721, 'max_depth': 2.1417028962862563, 'min_child_weight': 18.583060771207833, 'n_estimators': 680.4583262354053, 'subsample': 0.9627203317910963}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.8047665847665848, 'params': {'colsample_bytree': 0.4386224808028997, 'gamma': 1.0905900518887306, 'learning_rate': 0.08171296601129528, 'max_depth': 10.49946745178665, 'min_child_weight': 15.037172698496972, 'n_estimators': 295.25610551356567, 'subsample': 0.6986901719863734}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.7921048321048321, 'params': {'colsample_bytree': 0.6554148922595329, 'gamma': 3.4085348503986603, 'learning_rate': 0.08239134265988993, 'max_depth': 6.725194378803723, 'min_child_weight': 18.852542798862185, 'n_estimators': 308.0714482953049, 'subsample': 0.8082446151113571}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.8096969696969694, 'params': {'colsample_bytree': 0.9991337809826765, 'gamma': 0.5126040322793047, 'learning_rate': 0.07260519432257369, 'max_depth': 9.423245502822606, 'min_child_weight': 15.847405956328881, 'n_estimators': 344.4967382859727, 'subsample': 0.6786490570865282}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.7996396396396395, 'params': {'colsample_bytree': 0.7568204139947721, 'gamma': 1.6957198216140679, 'learning_rate': 0.08910434930732963, 'max_depth': 9.83411513542371, 'min_child_weight': 16.766247000274912, 'n_estimators': 296.4392957642451, 'subsample': 0.7339859087395934}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.7818673218673218, 'params': {'colsample_bytree': 0.5610267887536429, 'gamma': 1.1996996934160125, 'learning_rate': 0.02740850606427315, 'max_depth': 7.510800735240114, 'min_child_weight': 15.483620311049576, 'n_estimators': 344.1790554324846, 'subsample': 0.5360642707460326}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.8029320229320227, 'params': {'colsample_bytree': 0.5753185322122544, 'gamma': 0.6104192904409659, 'learning_rate': 0.07794566767931115, 'max_depth': 10.661693041374987, 'min_child_weight': 15.166568032093888, 'n_estimators': 345.6884238073195, 'subsample': 0.5738861631573046}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.7809009009009007, 'params': {'colsample_bytree': 0.5977452356218056, 'gamma': 2.181171509025215, 'learning_rate': 0.025746171359283754, 'max_depth': 9.422148060059264, 'min_child_weight': 13.848714200803009, 'n_estimators': 296.0551518873717, 'subsample': 0.6691247488919205}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8020147420147421, 'params': {'colsample_bytree': 0.8232262911273784, 'gamma': 0.8597224132129916, 'learning_rate': 0.06797939204777481, 'max_depth': 11.690097743196137, 'min_child_weight': 14.698653178822452, 'n_estimators': 293.2718494117894, 'subsample': 0.7057988830404347}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.8017854217854219, 'params': {'colsample_bytree': 0.5381698849396862, 'gamma': 1.0831605566108242, 'learning_rate': 0.06912522485869778, 'max_depth': 10.688165596060468, 'min_child_weight': 17.512951243479407, 'n_estimators': 342.3295926870487, 'subsample': 0.7896875721300407}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: {0}\\nParams: {1}\".format(xgb_bo_500.max[\"target\"], xgb_bo_500.max[\"params\"]))\n",
    "for i, res in enumerate(xgb_bo_500.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:50:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.461702448981613,\n              gamma=1.161953091628986, gpu_id=-1, importance_type='gain',\n              interaction_constraints='', learning_rate=0.05224857007800442,\n              max_delta_step=0, max_depth=8, min_child_weight=8.07481593002973,\n              missing=nan, monotone_constraints='()', n_estimators=302,\n              n_jobs=16, num_parallel_tree=1, random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=0.8855904253753344,\n              tree_method='exact', validate_parameters=1, verbosity=None)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params_500 = xgb_bo_500.max[\"params\"]\n",
    "params_xgb500 = {'colsample_bytree': opt_params_500[\"colsample_bytree\"],\n",
    "                 'gamma': opt_params_500[\"gamma\"],\n",
    "                 'learning_rate': opt_params_500[\"learning_rate\"],\n",
    "                 'max_depth': int(opt_params_500[\"max_depth\"]),\n",
    "                 'min_child_weight': opt_params_500[\"min_child_weight\"],\n",
    "                 'n_estimators': int(opt_params_500[\"n_estimators\"]),\n",
    "                 'subsample': opt_params_500[\"subsample\"]}\n",
    "\n",
    "clf_500 = XGBClassifier(**params_xgb500, n_jobs=16)\n",
    "\n",
    "clf_500.fit(X_train_500, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"datasets/test.csv\")\n",
    "X_test, y_test = test_df[test_df.columns.difference([\"patient_ID\", \"posOutcome\"])], test_df[\"posOutcome\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def calc_scores(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall_0, recall_1 = recall_score(y_test, y_pred, pos_label=0), recall_score(y_test, y_pred, pos_label=1)\n",
    "    precision_0, precision_1 =  precision_score(y_test, y_pred, pos_label=0), precision_score(y_test, y_pred, pos_label=1)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    return np.array([[acc, precision_0, recall_0, precision_1, recall_1,auc_score]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.765297\nrecall_0             0.785311\nprecision_0          0.774373\nrecall_1             0.745283\nprecision_1          0.757188\nauc                  0.845859\ndtype: float64"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_500 = X_test[ft_500]\n",
    "\n",
    "test_scores = calc_scores(clf_500, X_test_500, y_test)\n",
    "scores_test_500_df = pd.DataFrame(data=test_scores, columns=[\"balanced_accuracy\", \"recall_0\", \"precision_0\", \"recall_1\", \"precision_1\", \"auc\"])\n",
    "scores_test_500_df.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "pam35_genes = [\"BAG1\", \"BIRC5\", \"BLVRA\", \"CCNB1\", \"CCNE1\", \"CDC20\", \"CDC6\", \"CDH3\", \"CENPF\", \"CEP55\", \"EGFR\", \"ERBB2\", \"ESR1\", \"EXO1\", \"FOXA1\", \"FOXC1\",  \"GRB7\", \"KIF2C\", \"KRT14\", \"KRT17\", \"KRT5\", \"MAPT\", \"MDM2\", \"MELK\", \"MIA\", \"MKI67\", \"MMP11\", \"MYBL2\", \"MYC\", \"PGR\", \"RRM2\", \"SFRP1\", \"SLC39A6\", \"TYMS\", \"UBE2C\"]\n",
    "\n",
    "X_pam35 = X_train[pam35_genes]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:49:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5285030615978858,\n              gamma=2.4910681761867757, gpu_id=-1, importance_type='gain',\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=7, min_child_weight=6.401699642327911, missing=nan,\n              monotone_constraints='()', n_estimators=308, n_jobs=16,\n              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n              scale_pos_weight=1, subsample=0.9705148953040089,\n              tree_method='exact', validate_parameters=1, verbosity=None)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pam_bo = bayesian_opt(X_pam35, y_train)\n",
    "opt_params_pam = xgb_pam_bo.max[\"params\"]\n",
    "params_pam35 = {'colsample_bytree': opt_params_pam[\"colsample_bytree\"],\n",
    "                 'gamma': opt_params_pam[\"gamma\"],\n",
    "                 'learning_rate': opt_params_pam[\"learning_rate\"],\n",
    "                 'max_depth': int(opt_params_pam[\"max_depth\"]),\n",
    "                 'min_child_weight': opt_params_pam[\"min_child_weight\"],\n",
    "                 'n_estimators': int(opt_params_pam[\"n_estimators\"]),\n",
    "                 'subsample': opt_params_pam[\"subsample\"]}\n",
    "\n",
    "clf_pam = XGBClassifier(**params_pam35, n_jobs=16)\n",
    "\n",
    "clf_pam.fit(X_pam35, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    73.121202\nprecision_0          74.438202\nrecall_0             74.858757\nprecision_1          71.835443\nrecall_1             71.383648\nauc                  79.729418\ndtype: float64"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pam35 = X_test[pam35_genes]\n",
    "\n",
    "test_scores_pam35 = calc_scores(clf_pam, X_pam35, y_test)\n",
    "scores_test_pam35_df = pd.DataFrame(data=test_scores_pam35, columns=[\"balanced_accuracy\", \"precision_0\", \"recall_0\", \"precision_1\",\"recall_1\", \"auc\"])\n",
    "scores_test_pam35_df.mean() * 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "def recall_0(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "def precision_0(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "scoring = {\"balanced_accuracy\": make_scorer(balanced_accuracy_score),\n",
    "           \"recall_0\": make_scorer(recall_0), \"precision_0\": make_scorer(precision_0),\n",
    "           \"recall_1\": make_scorer(recall_score), \"precision_1\": make_scorer(precision_score), \"auc\": \"roc_auc\" }\n",
    "\n",
    "#cross_validation\n",
    "\n",
    "def print_score_comparison(raw_score, emb_score, target_feature=\"posOutcome\",\n",
    "                           header_1=\"Raw Score\", header_2=\"Embedding Score\"):\n",
    "    print(\"\\t\\t{0}\\n\\t\\t\\t{1}\\t\\t{2}\".format(target_feature, header_1, header_2))\n",
    "    print(\"\\t\\t-----------------------------------------------\")\n",
    "    print(\"balanced_accuracy:\\t{0:.3%}\\t\\t\\t{1:.3%}\\n\".format(raw_score[\"balanced_accuracy\"].mean(), emb_score[\"balanced_accuracy\"].mean()))\n",
    "    print(\"precision_0:\\t\\t{0:.3%}\\t\\t\\t{1:.3%}\\n\".format(raw_score[\"precision_0\"].mean(), emb_score[\"precision_0\"].mean()))\n",
    "    print(\"recall_0:\\t\\t{0:.3%}\\t\\t\\t{1:.3%}\\n\".format(raw_score[\"recall_0\"].mean(), emb_score[\"recall_0\"].mean()))\n",
    "    print(\"precision_1:\\t\\t{0:.3%}\\t\\t\\t{1:.3%}\\n\".format(raw_score[\"precision_1\"].mean(), emb_score[\"precision_1\"].mean()))\n",
    "    print(\"recall_1:\\t\\t{0:.3%}\\t\\t\\t{1:.3%}\\n\".format(raw_score[\"recall_1\"].mean(), emb_score[\"recall_1\"].mean()))\n",
    "    print(\"auc:\\t\\t\\t{0:.3%}\\t\\t\\t{1:.3%}\\n\".format(raw_score[\"auc\"].mean(), emb_score[\"auc\"].mean()))\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def param_tuning(X, y, n_folds=5, param_comb=25, scoring='roc_auc', jobs=12):\n",
    "    xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    rand_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring=scoring, n_jobs=jobs,\n",
    "                                   cv=skf.split(X, y), verbose=3, random_state=42)\n",
    "\n",
    "    start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "    rand_search.fit(X, y)\n",
    "    timer(start_time)\n",
    "    print(\"Best Score: {:.3%}\".format(rand_search.best_score_))\n",
    "    print(rand_search.best_params_)\n",
    "    return rand_search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[16:53:34] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:53:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 5 minutes and 22.97 seconds.\n",
      "Best Score: 91.218%\n",
      "{'subsample': 0.6, 'n_estimators': 700, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.03, 'gamma': 0.5, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=12)]: Done 125 out of 125 | elapsed:  4.9min finished\n"
     ]
    }
   ],
   "source": [
    "rand_params_500 = param_tuning(X_train_500, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([97.65030217, 96.3224237 , 97.5036397 , 96.75780582, 97.38839674]),\n 'score_time': array([0.04465437, 0.10535979, 0.06776261, 0.07968521, 0.0639739 ]),\n 'test_balanced_accuracy': array([0.81683047, 0.84176904, 0.82264537, 0.81891892, 0.84316134]),\n 'test_recall_0': array([0.83636364, 0.87272727, 0.79393939, 0.8       , 0.84848485]),\n 'test_precision_0': array([0.82142857, 0.8372093 , 0.85620915, 0.84615385, 0.85365854]),\n 'test_recall_1': array([0.7972973 , 0.81081081, 0.85135135, 0.83783784, 0.83783784]),\n 'test_precision_1': array([0.8137931 , 0.85106383, 0.7875    , 0.78980892, 0.83221477]),\n 'test_auc': array([0.91502867, 0.92674038, 0.91216216, 0.90266175, 0.91511057])}"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_500_rand = {'subsample': 0.6,\n",
    " 'n_estimators': 700,\n",
    " 'min_child_weight': 5,\n",
    " 'max_depth': 5,\n",
    " 'learning_rate': 0.03,\n",
    " 'gamma': 0.5,\n",
    " 'colsample_bytree': 0.8}\n",
    "\n",
    "clf_500_rand = XGBClassifier(**params_500_rand)\n",
    "\n",
    "s_v = StratifiedKFold(n_splits=5, shuffle=True, random_state=rand_seed)\n",
    "cv_results = cross_validate(clf_500_rand, X_train_500, y_train, scoring=scoring,\n",
    "                            verbose=2, n_jobs=-1)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "score_cols = [\"test_balanced_accuracy\",\"test_precision_0\", \"test_recall_0\",\n",
    "               \"test_precision_1\",\"test_recall_1\", \"test_auc\"]\n",
    "\n",
    "def get_scores(cv_results, score_keys=score_cols):\n",
    "    scores = np.empty([1, len(score_keys)])\n",
    "    for i, s in enumerate(score_keys):\n",
    "        scores[0][i] = np.mean(cv_results[s])\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.82866503, 0.84293188, 0.83030303, 0.81487612, 0.82702703,\n        0.9143407 ]])"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_rand_500 = get_scores(cv_results)\n",
    "scores_rand_500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:11:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.76105959, 0.77247191, 0.77683616, 0.75      , 0.74528302,\n        0.85118857]])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_500_rand.fit(X_train_500, y_train)\n",
    "test_scores_rand_500 = calc_scores(clf_500_rand, X_test_500, y_test)\n",
    "test_scores_rand_500_df = pd.DataFrame(data=test_scores_rand_500, columns=[\"balanced_accuracy\", \"recall_0\", \"precision_0\", \"recall_1\", \"precision_1\", \"auc\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "pam35_pipe = make_pipeline(ColumnSelector(cols=pam35_genes),\n",
    "                           clf_pam)\n",
    "\n",
    "clf_500_pipe = make_pipeline(ColumnSelector(cols=ft_500),\n",
    "                             clf_500_rand)\n",
    "\n",
    "sclf_1 = StackingCVClassifier(classifiers=[clf_500_pipe, pam35_pipe], meta_classifier=LogisticRegression(),\n",
    "                              cv=s_v, use_clones=False, verbose=True,\n",
    "                              n_jobs=14,\n",
    "                              random_state=rand_seed)\n",
    "\n",
    "# sclf_1.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.82824734, 0.83824082, 0.83757576, 0.82005687, 0.81891892,\n        0.84077805]])"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_sclf_1 = cross_validate(sclf_1, X_train, y_train, n_jobs=-1,\n",
    "                             cv=s_v, scoring=scoring,\n",
    "                             verbose=2)\n",
    "scores_sclf_1 = get_scores(scores_sclf_1)\n",
    "scores_sclf_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.83195332, 0.84001719, 0.84363636, 0.82591047, 0.82027027,\n        0.91352989]])"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses50_genes = [\"PRND\", \"FRS3\", \"FCN3\", \"DSCR4\", \"BRCA2\", \"CXCL6\", \"LMX1B\", \"DLX5\", \"OMP\", \"ADH6\", \"PGAP1\", \"ART3\", \"BCHE\", \"FGB\", \"IL1RAPL1\", \"FSTL4\", \"ASGR1\", \"ZNF135\", \"DLL3\", \"NPHS2\", \"ANGPT2\", \"GLP2R\", \"GRIA3\", \"HOXB8\", \"MSC\", \"PLA2R1\", \"CYP2F1\", \"TAS2R7\", \"NKX6-1\", \"WNT11\", \"CHST11\", \"CLCA4\", \"ENPEP\", \"PAH\", \"WFDC1\", \"CHGA\", \"SEZ6L\", \"UGT2A3\", \"PRDM16\", \"GALR2\", \"GUCA1A\", \"CASQ1\", \"NOS1AP\", \"CACNA2D3\", \"FHOD3\", \"SRGAP3\", \"TMOD2\", \"ATOH1\", \"SLC6A1\", \"HAS1\"]\n",
    "clf_moses50 = XGBClassifier()\n",
    "clf_moses50.load_model(\"datasets/models/moses50_raw.json\")\n",
    "moses50_pipe = make_pipeline(ColumnSelector(cols=moses50_genes),\n",
    "                           clf_moses50)\n",
    "\n",
    "sclf_2 = StackingCVClassifier(classifiers=[clf_500_pipe, pam35_pipe, moses50_pipe],\n",
    "                  meta_classifier=LogisticRegression(),\n",
    "                  cv=s_v, use_clones=False, verbose=True,\n",
    "                  n_jobs=14, use_probas=True,\n",
    "                  random_state=rand_seed)\n",
    "scores_sclf_2 = cross_validate(sclf_2, X_train, y_train, n_jobs=-1,\n",
    "                             cv=s_v, scoring=scoring,\n",
    "                             verbose=2)\n",
    "scores_sclf_2 = get_scores(scores_sclf_2)\n",
    "scores_sclf_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 classifiers...\n",
      "Fitting classifier1: pipeline (1/2)\n",
      "Fitting classifier2: pipeline (2/2)\n",
      "[10:21:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:21:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=14)]: Done   2 out of   5 | elapsed:  2.0min remaining:  3.0min\n",
      "[Parallel(n_jobs=14)]: Done   5 out of   5 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=14)]: Done   2 out of   5 | elapsed:  1.4min remaining:  2.1min\n",
      "[Parallel(n_jobs=14)]: Done   5 out of   5 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "balanced_accuracy    0.761060\nrecall_0             0.772472\nprecision_0          0.776836\nrecall_1             0.750000\nprecision_1          0.745283\nauc                  0.794443\ndtype: float64"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf_1.fit(X_train, y_train)\n",
    "test_scors_sclf_1 = calc_scores(sclf_1, X_test, y_test)\n",
    "test_scors_sclf_1_df = pd.DataFrame(data=test_scors_sclf_1, columns=[\"balanced_accuracy\", \"recall_0\", \"precision_0\", \"recall_1\", \"precision_1\", \"auc\"])\n",
    "test_scors_sclf_1_df.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.762632\nrecall_0             0.774648\nprecision_0          0.776836\nrecall_1             0.750789\nprecision_1          0.748428\nauc                  0.848977\ndtype: float64"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sclf_2.fit(X_train, y_train)\n",
    "test_scors_sclf_2 = calc_scores(sclf_2, X_test, y_test)\n",
    "test_scors_sclf_2_df = pd.DataFrame(data=test_scors_sclf_2, columns=[\"balanced_accuracy\", \"recall_0\", \"precision_0\", \"recall_1\", \"precision_1\", \"auc\"])\n",
    "test_scors_sclf_2_df.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "mrmr_df = train_df[train_df.columns.difference([\"patient_ID\"])]\n",
    "pos_outcome = train_df[\"posOutcome\"]\n",
    "mrmr_df.drop(labels=[\"posOutcome\"], inplace=True, axis=1)\n",
    "mrmr_df.insert(0, \"posOutcome\", pos_outcome)\n",
    "mrmr_df.to_csv(\"datasets/train_mrmr.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}