{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from mlxtend.classifier import StackingCVClassifier, EnsembleVoteClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "params = {'n_estimators': [300, 400, 500, 600, 700],\n",
    "              'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07],\n",
    "              'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "              'max_depth': [3, 4, 5, 6],\n",
    "              'subsample': [0.6, 0.8, 1.0],\n",
    "              'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "              'min_child_weight': [1, 2, 3, 4, 5]}\n",
    "\n",
    "seed = 42\n",
    "st_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "def calc_scores(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall_0, recall_1 = recall_score(y_test, y_pred, pos_label=0), recall_score(y_test, y_pred, pos_label=1)\n",
    "    precision_0, precision_1 =  precision_score(y_test, y_pred, pos_label=0), precision_score(y_test, y_pred, pos_label=1)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    arr = np.array([[acc, precision_0, recall_0, precision_1, recall_1,auc_score]])\n",
    "    return pd.DataFrame(data=arr, columns=[\"balanced_accuracy\", \"recall_0\", \"precision_0\", \"recall_1\", \"precision_1\", \"auc\"])\n",
    "\n",
    "def recall_0(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "def precision_0(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "scoring = {\"balanced_accuracy\": make_scorer(balanced_accuracy_score),\n",
    "           \"recall_0\": make_scorer(recall_0), \"precision_0\": make_scorer(precision_0),\n",
    "           \"recall_1\": make_scorer(recall_score), \"precision_1\": make_scorer(precision_score), \"auc\": \"roc_auc\" }\n",
    "\n",
    "#cross_validation\n",
    "\n",
    "def print_score_comparison(raw_score, emb_score, target_feature=\"posOutcome\",\n",
    "                           header_1=\"Raw Score\", header_2=\"Embedding Score\"):\n",
    "    print(\"\\t\\t{0}\\n\\t\\t\\t{1}\\t\\t{2}\".format(target_feature, header_1, header_2))\n",
    "    print(\"\\t\\t-------------------------------------------------------\")\n",
    "    print(\"balanced_accuracy:\\t{0:.2%}\\t\\t\\t\\t{1:.2%}\\n\".format(raw_score[\"balanced_accuracy\"].mean(), emb_score[\"balanced_accuracy\"].mean()))\n",
    "    print(\"recall_0:\\t\\t{0:.2%}\\t\\t\\t\\t{1:.2%}\\n\".format(raw_score[\"recall_0\"].mean(), emb_score[\"recall_0\"].mean()))\n",
    "    print(\"precision_0:\\t\\t{0:.2%}\\t\\t\\t\\t{1:.2%}\\n\".format(raw_score[\"precision_0\"].mean(), emb_score[\"precision_0\"].mean()))\n",
    "    print(\"recall_1:\\t\\t{0:.2%}\\t\\t\\t\\t{1:.2%}\\n\".format(raw_score[\"recall_1\"].mean(), emb_score[\"recall_1\"].mean()))\n",
    "    print(\"precision_1:\\t\\t{0:.2%}\\t\\t\\t\\t{1:.2%}\\n\".format(raw_score[\"precision_1\"].mean(), emb_score[\"precision_1\"].mean()))\n",
    "    print(\"auc:\\t\\t\\t{0:.2%}\\t\\t\\t\\t{1:.2%}\\n\".format(raw_score[\"auc\"].mean(), emb_score[\"auc\"].mean()))\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def param_tuning(X, y, n_folds=5, param_comb=25, scoring='roc_auc', jobs=12):\n",
    "    xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    rand_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring=scoring, n_jobs=jobs,\n",
    "                                   cv=skf.split(X, y), verbose=3, random_state=42)\n",
    "\n",
    "    start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "    rand_search.fit(X, y)\n",
    "    timer(start_time)\n",
    "    print(\"Best Score: {:.3%}\".format(rand_search.best_score_))\n",
    "    print(rand_search.best_params_)\n",
    "    return rand_search\n",
    "\n",
    "score_cols = [\"test_balanced_accuracy\",\"test_precision_0\", \"test_recall_0\",\n",
    "               \"test_precision_1\",\"test_recall_1\", \"test_auc\"]\n",
    "\n",
    "def get_scores(cv_results, score_keys=None, df_cols=None):\n",
    "    if score_keys is None:\n",
    "        score_keys = score_cols\n",
    "    if df_cols is None:\n",
    "        score_keys = score_cols\n",
    "    scores = np.empty([1, len(score_keys)])\n",
    "    for i, s in enumerate(score_keys):\n",
    "        scores[0][i] = np.mean(cv_results[s])\n",
    "    scores_df = pd.DataFrame(data=scores, columns=df_cols)\n",
    "    return scores_df\n",
    "\n",
    "def evaluate_embedding(path, outcome_df, target=\"posOutcome\", merge_col=\"patient_ID\", n_jobs=-1):\n",
    "    emb_df = pd.read_csv(path, sep=\"\\t\")\n",
    "    emb_outcome_df = pd.merge(outcome_df, emb_df, on=merge_col)\n",
    "    X_emb, y_emb = emb_outcome_df[emb_outcome_df.columns.difference([merge_col, target])], emb_outcome_df[target]\n",
    "    X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(X_emb, y_emb, test_size=0.3, random_state=seed)\n",
    "    rand_search_emb = param_tuning(X_train_emb, y_train_emb, jobs=n_jobs)\n",
    "    params = rand_search_emb.best_params_\n",
    "    clf_emb = rand_search_emb.best_estimator_\n",
    "    cv_res = cross_validate(clf_emb, X_train_emb, y_train_emb, scoring=scoring, n_jobs=n_jobs, verbose=1, return_train_score=True,\n",
    "                            cv=st_cv)\n",
    "    cv_res_df = get_scores(cv_res)\n",
    "    clf_emb.fit(X_train_emb, y_train_emb)\n",
    "    test_scores_df = calc_scores(clf_emb, X_test_emb, y_test_emb)\n",
    "\n",
    "    return params, cv_res_df, test_scores_df\n",
    "\n",
    "def load_features(path):\n",
    "    feats = []\n",
    "    with open(path, \"r\") as fp:\n",
    "        for line in fp.readlines():\n",
    "            feats.append(line.strip())\n",
    "\n",
    "    return feats\n",
    "def evaluate_ge(x_train, y_train, x_test, y_test, outcome_cols=None, feats=None, jobs=-1,\n",
    "                scoring=scoring, rand_scoring=\"roc_auc\", target=\"posOutcome\"):\n",
    "    if feats is not None:\n",
    "        if outcome_cols is not None:\n",
    "            cols = outcome_cols + feats\n",
    "        else:\n",
    "            cols = feats\n",
    "        x_train = x_train[cols]\n",
    "        x_test = x_test[cols]\n",
    "    rand_search = param_tuning(x_train, y_train, scoring=rand_scoring, jobs=jobs)\n",
    "    params = rand_search.best_params_\n",
    "    clf = XGBClassifier(**params)\n",
    "    cv_res = cross_validate(clf, x_train, y_train,scoring=scoring, cv=st_cv, n_jobs=jobs)\n",
    "\n",
    "    cv_res_df = get_scores(cv_res, score_cols, df_cols=[\"balanced_accuracy\", \"recall_0\", \"precision_0\", \"recall_1\", \"precision_1\", \"auc\"])\n",
    "    clf.fit(x_train, y_train)\n",
    "    test_scores_df = calc_scores(clf, x_test, y_test)\n",
    "\n",
    "    return params, clf, cv_res_df, test_scores_df\n",
    "\n",
    "def discretize_dataset(X, features, bins_labels = None):\n",
    "    if bins_labels is None:\n",
    "\t    bins_labels = [-1, 0, 1]\n",
    "    X_disc = X[features]\n",
    "    bin_dict = {}\n",
    "\n",
    "    for ft in features:\n",
    "        r1 = X_disc[ft].mean() - X_disc[ft].std() / 2\n",
    "        r2 = X_disc[ft].mean() + X_disc[ft].std() / 2\n",
    "        bin_dict[ft]= [-np.inf, r1, r2, np.inf]\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    le.fit(bins_labels)\n",
    "\n",
    "    for ft in bin_dict:\n",
    "        X_disc[ft] = le.transform(pd.cut(X_disc[ft], bins=bin_dict[ft], labels=bins_labels))\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    transformed = ohe.fit_transform(X_disc).toarray()\n",
    "    X_disc = pd.DataFrame(transformed, columns=ohe.get_feature_names(features))\n",
    "    return X_disc\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "def optimize_k_v1(df, target, exclude=None):\n",
    "    if exclude is None:\n",
    "        exclude = [\"patient_ID\"]\n",
    "    df = df.drop(exclude, axis=1)\n",
    "    data = df.to_numpy()\n",
    "    errors = []\n",
    "    for k in range(1, 20, 2):\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed = imputer.fit_transform(data)\n",
    "        df_imputed = pd.DataFrame(imputed, columns=df.columns)\n",
    "\n",
    "        X = df_imputed.drop(target, axis=1)\n",
    "        y = df_imputed[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = rmse(y_test, preds)\n",
    "        errors.append({'K': k, 'RMSE': error})\n",
    "\n",
    "    return errors\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "def optimize_k_v2(df, target, exclude=None):\n",
    "    if exclude is None:\n",
    "        exclude = [\"patient_ID\", target]\n",
    "    X = df.drop(exclude, axis=1)\n",
    "    data = X.to_numpy()\n",
    "    errors = []\n",
    "    for k in range(1, 20, 2):\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed = imputer.fit_transform(data)\n",
    "        df_imputed = pd.DataFrame(imputed, columns=X.columns)\n",
    "\n",
    "        y = df[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = rmse(y_test, preds)\n",
    "        errors.append({'K': k, 'RMSE': error})\n",
    "\n",
    "    print(errors)\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in errors:\n",
    "        x.append(i[\"K\"])\n",
    "        y.append(i[\"RMSE\"])\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel(\"K\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(\"RMSE at d/t K values\")\n",
    "    plt.xticks(np.arange(min(x), max(x) + 1, 2))\n",
    "    fig.show()\n",
    "\n",
    "def impute_dataset(df, imputer, target=\"posOutcome\"):\n",
    "    X = df.drop([\"patient_ID\", target], axis=1)\n",
    "    X_new = imputer.fit_transform(X)\n",
    "    df_imputed = pd.DataFrame(X_new, columns=X.columns)\n",
    "    p_outcome_df = df[[\"patient_ID\", \"posOutcome\"]]\n",
    "    df_imputed = pd.concat([p_outcome_df, df_imputed], axis=1)\n",
    "    return df_imputed\n",
    "\n",
    "def one_hot_encode(df, cat_features):\n",
    "    for i in cat_features:\n",
    "        df[i] = df[i].astype(dtype=np.int64)\n",
    "    X_cats = df[cat_features]\n",
    "    ohe = OneHotEncoder()\n",
    "    X_ohe = ohe.fit_transform(X_cats).toarray()\n",
    "    fts_names = ohe.get_feature_names(cat_features)\n",
    "    ohe_df = pd.DataFrame(X_ohe, columns=fts_names)\n",
    "    df_encoded = pd.concat([df.drop(cat_features, axis=1), ohe_df], axis=1)\n",
    "    return df_encoded\n",
    "\n",
    "def find_diff(df1, df2, index=\"patient_ID\"):\n",
    "    def highlight_diff(data, color='yellow'):\n",
    "        attr = 'background-color: {}'.format(color)\n",
    "        other = data.xs('First', axis='columns', level=-1)\n",
    "        return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
    "                            index=data.index, columns=data.columns)\n",
    "\n",
    "    df_all = pd.concat([df1.set_index(index), df2.set_index(index)],\n",
    "                   axis='columns', keys=['First', 'Second'])\n",
    "    df_final = df_all.swaplevel(axis='columns')[df1.columns[1:]]\n",
    "    df_final.style.apply(highlight_diff, axis=None)\n",
    "    return df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "#Load the datasets\n",
    "\n",
    "ge_df = pd.read_csv(\"datasets/merged-combat15.csv\")\n",
    "state_df = pd.read_csv(\"datasets/state_and_outcome.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "\n",
    "state_df = state_df.drop([\"series_id\", \"channel_count\", \"RFS\", \"DFS\",\n",
    "                          \"pCR\", \"posOutcome2\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPL570' 'GPL96' 'GPL1708,GPL4133' 'GPL5049' 'GPL1223' 'GPL5325']\n",
      "['LumB' 'Her2' 'Basal' nan 'LumA' 'Normal']\n",
      "['k5' 'k3' 'k1' 'k2' 'k4' nan]\n",
      "['T3' 'T2' 'T4' 'T1' nan 'T0']\n"
     ]
    }
   ],
   "source": [
    "gpl_vals = state_df[\"gpl\"].unique()\n",
    "print(gpl_vals)\n",
    "pam_subtypes = state_df[\"pam_coincide\"].unique()\n",
    "print(pam_subtypes)\n",
    "p5_types = state_df[\"p5\"].unique()\n",
    "print(p5_types)\n",
    "tumor_types = state_df[\"tumor\"].unique()\n",
    "print(tumor_types)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "state_df = state_df.dropna(axis=0, subset=[\"pam_coincide\", \"p5\"])\n",
    "state_df = state_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "state_df[\"tumor\"] = state_df[\"tumor\"].astype(\"category\").cat.codes\n",
    "state_df[\"pam_coincide\"] = pam_code_df = state_df[\"pam_coincide\"].astype(\"category\").cat.codes\n",
    "state_df[\"p5\"] = p5_code_df = state_df[\"p5\"].astype(\"category\").cat.codes\n",
    "state_df[\"gpl\"] = gpl_code = state_df[\"gpl\"].astype(\"category\").cat.codes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=9)\n",
    "state_df_v2 = impute_dataset(state_df, knn_imputer)\n",
    "state_df_v2 = state_df_v2.drop([\"gpl\"], axis=1)\n",
    "state_df_v2 = one_hot_encode(state_df_v2, [\"pam_coincide\", \"p5\"])\n",
    "state_df_v2 = state_df_v2.astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "X_st_v2, y_st_v2 = state_df_v2.drop([\"posOutcome\"], axis=1), state_df_v2[\"posOutcome\"]\n",
    "X_st_v2_train, X_st_v2_test, y_st_v2_train, y_st_v2_test = train_test_split(X_st_v2, y_st_v2,                                            test_size=0.3, random_state=seed, stratify=y_st_v2)\n",
    "\n",
    "X_st_v2_train.to_csv(\"datasets/train_st_knn.csv\", index=False)\n",
    "X_st_v2_test.to_csv(\"datasets/test_st_knn.csv\", index=False)\n",
    "\n",
    "X_st_v2_train = X_st_v2_train.drop([\"patient_ID\"], axis=1)\n",
    "X_st_v2_test = X_st_v2_test.drop([\"patient_ID\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[14:52:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:52:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 6.52 seconds.\n",
      "Best Score: 76.221%\n",
      "{'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.03, 'gamma': 2, 'colsample_bytree': 0.6}\n",
      "[14:52:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "balanced_accuracy    0.701849\nrecall_0             0.700725\nprecision_0          0.766789\nrecall_1             0.713096\nprecision_1          0.636909\nauc                  0.762209\ndtype: float64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_st_v2, clf_st_v2, cv_scores_st_v2, test_scores_st_v2 = evaluate_ge(\n",
    "    X_st_v2_train, y_st_v2_train, X_st_v2_test, y_st_v2_test)\n",
    "cv_scores_st_v2.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.712924\nrecall_0             0.707379\nprecision_0          0.789773\nrecall_1             0.730909\nprecision_1          0.636076\nauc                  0.785691\ndtype: float64"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores_st_v2.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "ge_state_outcome_df_v2 = pd.merge(state_df_v2, ge_df, on=\"patient_ID\")\n",
    "X_st_v2_ge, y_st_v2_ge = ge_state_outcome_df_v2.drop([\"patient_ID\", \"posOutcome\"], axis=1), ge_state_outcome_df_v2[\"posOutcome\"]\n",
    "\n",
    "X_train_st_v2_ge, X_test_st_v2_ge, y_train_st_v2_ge, y_test_st_v2_ge = train_test_split(X_st_v2_ge, y_st_v2_ge, test_size=0.3, stratify=y_st_v2_ge, random_state=seed)\n",
    "ft_mrmr50 = load_features(\"datasets/mrmr_ft50.txt\")\n",
    "X_train_st_v2_ge = X_train_st_v2_ge[ft_mrmr50]\n",
    "X_test_st_v2_ge = X_test_st_v2_ge[ft_mrmr50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:00:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:00:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 23.56 seconds.\n",
      "Best Score: 82.483%\n",
      "{'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.07, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:01:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   22.2s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_st_v2_ge, clf_st_v2_ge, cv_scores_st_v2_ge, test_scores_st_v2_ge = evaluate_ge(\n",
    "    X_train_st_v2_ge, y_train_st_v2_ge, X_test_st_v2_ge, y_test_st_v2_ge)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.760287\nrecall_0             0.773307\nprecision_0          0.775363\nrecall_1             0.749456\nprecision_1          0.745211\nauc                  0.824833\ndtype: float64"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores_st_v2_ge.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.767459\nrecall_0             0.757106\nprecision_0          0.832386\nrecall_1             0.790036\nprecision_1          0.702532\nauc                  0.834418\ndtype: float64"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores_st_v2_ge.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "sim_imputer_1 = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "state_df_v3 = impute_dataset(state_df, sim_imputer_1)\n",
    "state_df_v3 = state_df_v3.drop([\"gpl\"], axis=1)\n",
    "state_df_v3 = one_hot_encode(state_df_v3, [\"pam_coincide\", \"p5\"])\n",
    "state_df_v3 = state_df_v3.astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "X_st_v3, y_st_v3 = state_df_v3.drop([\"posOutcome\"], axis=1), state_df_v3[\"posOutcome\"]\n",
    "X_st_v3_train, X_st_v3_test, y_st_v3_train, y_st_v3_test = train_test_split(X_st_v3, y_st_v3,                                            test_size=0.3, random_state=seed, stratify=y_st_v3)\n",
    "\n",
    "X_st_v3_train.to_csv(\"datasets/train_st_sim.csv\", index=False)\n",
    "X_st_v3_test.to_csv(\"datasets/test_st_sim.csv\", index=False)\n",
    "\n",
    "X_st_v3_train = X_st_v3_train.drop([\"patient_ID\"], axis=1)\n",
    "X_st_v3_test = X_st_v3_test.drop([\"patient_ID\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:05:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:05:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 5.5 seconds.\n",
      "Best Score: 76.270%\n",
      "{'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.03, 'gamma': 2, 'colsample_bytree': 0.6}\n",
      "[15:05:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "balanced_accuracy    0.703753\nrecall_0             0.702253\nprecision_0          0.769235\nrecall_1             0.715323\nprecision_1          0.638270\nauc                  0.762701\ndtype: float64"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_st_v3, clf_st_v3, cv_scores_st_v3, test_scores_st_v3 = evaluate_ge(\n",
    "    X_st_v3_train, y_st_v3_train, X_st_v3_test, y_st_v3_test)\n",
    "cv_scores_st_v3.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.702172\nrecall_0             0.696970\nprecision_0          0.784091\nrecall_1             0.720588\nprecision_1          0.620253\nauc                  0.785511\ndtype: float64"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores_st_v3.mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tposOutcome\n",
      "\t\t\tState KNN\t\tState Simple\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t71.29%\t\t\t\t70.22%\n",
      "\n",
      "recall_0:\t\t70.74%\t\t\t\t69.70%\n",
      "\n",
      "precision_0:\t\t78.98%\t\t\t\t78.41%\n",
      "\n",
      "recall_1:\t\t73.09%\t\t\t\t72.06%\n",
      "\n",
      "precision_1:\t\t63.61%\t\t\t\t62.03%\n",
      "\n",
      "auc:\t\t\t78.57%\t\t\t\t78.55%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score_comparison(test_scores_st_v2, test_scores_st_v3, header_1=\"State KNN\", header_2=\"State Simple\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "\n",
    "ge_state_outcome_df_v3 = pd.merge(state_df_v3, ge_df, on=\"patient_ID\")\n",
    "X_st_v3_ge, y_st_v3_ge = ge_state_outcome_df_v3.drop([\"patient_ID\", \"posOutcome\"], axis=1), ge_state_outcome_df_v3[\"posOutcome\"]\n",
    "\n",
    "X_train_st_v3_ge, X_test_st_v3_ge, y_train_st_v3_ge, y_test_st_v3_ge = train_test_split(X_st_v3_ge, y_st_v3_ge, test_size=0.3, stratify=y_st_v3_ge, random_state=seed)\n",
    "ft_mrmr50 = load_features(\"datasets/mrmr_ft50.txt\")\n",
    "X_train_st_v3_ge = X_train_st_v3_ge[ft_mrmr50]\n",
    "X_test_st_v3_ge = X_test_st_v3_ge[ft_mrmr50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:08:59] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:08:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 23.25 seconds.\n",
      "Best Score: 82.483%\n",
      "{'subsample': 0.8, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.07, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:10:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   21.9s finished\n"
     ]
    }
   ],
   "source": [
    "params_st_v3_ge, clf_st_v3_ge, cv_scores_st_v3_ge, test_scores_st_v3_ge = evaluate_ge(\n",
    "    X_train_st_v3_ge, y_train_st_v3_ge, X_test_st_v3_ge, y_test_st_v3_ge)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.760287\nrecall_0             0.773307\nprecision_0          0.775363\nrecall_1             0.749456\nprecision_1          0.745211\nauc                  0.824833\ndtype: float64"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores_st_v3_ge.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy    0.767459\nrecall_0             0.757106\nprecision_0          0.832386\nrecall_1             0.790036\nprecision_1          0.702532\nauc                  0.834418\ndtype: float64"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores_st_v3_ge.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "           posOutcome           ER         HER2           PR         node  \\\n                First Second First Second First Second First Second First   \npatient_ID                                                                  \n809184              0      0     1      1     0      0     0      0     1   \n809185              1      1     0      0     0      0     0      0     1   \n809186              0      0     0      0     0      0     0      0     1   \n809187              0      0     1      1     1      1     1      1     1   \n809188              0      0     1      1     0      0     1      1     1   \n...               ...    ...   ...    ...   ...    ...   ...    ...   ...   \n491199              1      1     0      0     0      0     0      0     0   \n491270              1      1     1      1     1      1     1      1     0   \n491200              1      1     0      0     1      1     0      0     1   \n491201              1      1     0      0     0      0     0      0     0   \n491202              1      1     0      0     0      0     0      0     0   \n\n                   ...  p5_0         p5_1         p5_2         p5_3         \\\n           Second  ... First Second First Second First Second First Second   \npatient_ID         ...                                                       \n809184          1  ...     0      0     0      0     0      0     0      0   \n809185          1  ...     0      0     0      0     0      0     0      0   \n809186          1  ...     0      0     0      0     1      1     0      0   \n809187          1  ...     0      0     0      0     0      0     0      0   \n809188          1  ...     1      1     0      0     0      0     0      0   \n...           ...  ...   ...    ...   ...    ...   ...    ...   ...    ...   \n491199          0  ...     0      0     1      1     0      0     0      0   \n491270          0  ...     0      0     0      0     0      0     0      0   \n491200          1  ...     0      0     1      1     0      0     0      0   \n491201          0  ...     0      0     0      0     0      0     1      1   \n491202          0  ...     0      0     1      1     0      0     0      0   \n\n            p5_4         \n           First Second  \npatient_ID               \n809184         1      1  \n809185         1      1  \n809186         0      0  \n809187         1      1  \n809188         0      0  \n...          ...    ...  \n491199         0      0  \n491270         1      1  \n491200         0      0  \n491201         0      0  \n491202         0      0  \n\n[2225 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">posOutcome</th>\n      <th colspan=\"2\" halign=\"left\">ER</th>\n      <th colspan=\"2\" halign=\"left\">HER2</th>\n      <th colspan=\"2\" halign=\"left\">PR</th>\n      <th colspan=\"2\" halign=\"left\">node</th>\n      <th>...</th>\n      <th colspan=\"2\" halign=\"left\">p5_0</th>\n      <th colspan=\"2\" halign=\"left\">p5_1</th>\n      <th colspan=\"2\" halign=\"left\">p5_2</th>\n      <th colspan=\"2\" halign=\"left\">p5_3</th>\n      <th colspan=\"2\" halign=\"left\">p5_4</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>...</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n      <th>First</th>\n      <th>Second</th>\n    </tr>\n    <tr>\n      <th>patient_ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>809184</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>809185</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>809186</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>809187</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>809188</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>491199</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>491270</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>491200</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>491201</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>491202</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2225 rows Ã— 32 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_diff(state_df_v2, state_df_v3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}