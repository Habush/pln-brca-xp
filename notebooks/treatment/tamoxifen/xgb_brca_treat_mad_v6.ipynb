{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xabush/pln-brca-xp/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd \"~/pln-brca-xp/notebooks\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import median_abs_deviation\n",
    "from pymrmre import mrmr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess_data(df, scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset\n",
    "    1. Applys median absolute deviation (MAD)\n",
    "    2. Selects the top 20% genes using MAD values\n",
    "    3. Apply scaling to be in (0, 1)\n",
    "    :param df: the pandas dataframe\n",
    "    :param scaler: The transformer to use for scaling, by default MinMaxSclaer\n",
    "    :return: a preprocessed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    mad_arr = median_abs_deviation(df, axis=0)\n",
    "    df_mad = pd.DataFrame(mad_arr.reshape(1, -1), columns=df.columns)\n",
    "    df_mad = df_mad.sort_values(by=0, ascending=False, axis=1)\n",
    "    num_cols = int(df_mad.shape[1] * 0.2)\n",
    "    high_var_genes = df_mad.iloc[:, :num_cols].columns.to_list()\n",
    "    df_final = df.loc[:, high_var_genes]\n",
    "    if scaler is None:\n",
    "        norm = MinMaxScaler()\n",
    "    else:\n",
    "        norm = scaler\n",
    "    x = norm.fit_transform(df_final)\n",
    "    df_final = pd.DataFrame(x, columns=df_final.columns, index=df_final.index)\n",
    "    return df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 1766)\n"
     ]
    },
    {
     "data": {
      "text/plain": "              TFAP2B   SCGB1D2     DHRS2   SCGB2A2      CPB1       LTF  \\\npatient_ID                                                               \n249296      0.312098  0.324304  0.385159  0.551446  0.107906  0.385246   \n249524      0.136203  0.208200  0.463417  0.525977  1.000000  0.207136   \n249527      0.126140  0.898438  0.141661  0.928443  0.476218  0.695349   \n249529      0.256343  0.763848  0.184840  0.799118  0.798501  0.425631   \n249530      0.471651  0.588570  0.474179  0.625999  0.000000  0.624659   \n...              ...       ...       ...       ...       ...       ...   \n305260      0.057840  0.221925  0.229260  0.124437  0.323815  0.186135   \n305261      0.196474  0.614002  0.238328  0.580901  0.314771  0.445792   \n305262      0.414299  0.682621  0.193703  0.636208  0.318074  0.296531   \n305263      0.468658  0.283043  0.201599  0.345914  0.318074  0.189903   \n305264      0.445972  0.609305  0.571654  0.587775  0.316866  0.432453   \n\n               S100P       PIP   SCGB2A1    CYP2B6  ...      DLG5  SERPINB1  \\\npatient_ID                                          ...                       \n249296      0.373817  0.502808  0.272663  0.768693  ...  0.582445  0.768956   \n249524      0.591409  0.439236  0.289637  1.000000  ...  0.595341  0.711269   \n249527      0.740925  0.408495  0.893788  0.894466  ...  0.737108  0.862702   \n249529      0.528707  0.434600  0.923083  0.842613  ...  0.626282  0.769154   \n249530      0.439110  0.381255  0.314408  0.634108  ...  0.655757  0.879676   \n...              ...       ...       ...       ...  ...       ...       ...   \n305260      0.082081  0.480966  0.147167  0.760505  ...  0.563893  0.658029   \n305261      0.404507  0.570273  0.546146  0.589412  ...  0.383422  0.095244   \n305262      0.442509  0.572413  0.624450  0.519011  ...  0.387024  0.595301   \n305263      0.183957  0.655451  0.199281  0.368795  ...  0.367569  0.387289   \n305264      0.330094  0.600416  0.517167  0.621371  ...  0.364670  0.453505   \n\n              SNAPC1     JOSD1     ALMS1       FUS     STAU2      GLG1  \\\npatient_ID                                                               \n249296      0.144002  0.659206  0.502714  0.802432  0.059775  0.797122   \n249524      0.146812  0.791981  0.524508  0.974547  0.434257  0.816539   \n249527      0.167977  0.834964  0.426567  0.925897  0.209239  0.828620   \n249529      0.140055  0.752708  0.393765  0.867743  0.306340  0.714705   \n249530      0.117141  0.752675  0.521737  0.941098  0.360808  0.852167   \n...              ...       ...       ...       ...       ...       ...   \n305260      0.193752  0.529967  0.427364  0.639396  0.374160  0.657968   \n305261      0.479955  0.345655  0.462728  0.492059  0.486635  0.463083   \n305262      0.443945  0.634357  0.475175  0.369180  0.589576  0.000000   \n305263      0.541773  0.595527  0.364533  0.336226  0.711637  0.022291   \n305264      0.529371  0.623269  0.400310  0.454424  0.397859  0.129941   \n\n              ZNF268      CTSO  \npatient_ID                      \n249296      0.130086  0.176577  \n249524      0.171427  0.124036  \n249527      0.140970  0.199764  \n249529      0.112959  0.174203  \n249530      0.162523  0.220044  \n...              ...       ...  \n305260      0.492918  0.832546  \n305261      0.605367  0.531432  \n305262      0.822313  0.296249  \n305263      0.851226  0.359374  \n305264      0.445945  0.462979  \n\n[649 rows x 1766 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TFAP2B</th>\n      <th>SCGB1D2</th>\n      <th>DHRS2</th>\n      <th>SCGB2A2</th>\n      <th>CPB1</th>\n      <th>LTF</th>\n      <th>S100P</th>\n      <th>PIP</th>\n      <th>SCGB2A1</th>\n      <th>CYP2B6</th>\n      <th>...</th>\n      <th>DLG5</th>\n      <th>SERPINB1</th>\n      <th>SNAPC1</th>\n      <th>JOSD1</th>\n      <th>ALMS1</th>\n      <th>FUS</th>\n      <th>STAU2</th>\n      <th>GLG1</th>\n      <th>ZNF268</th>\n      <th>CTSO</th>\n    </tr>\n    <tr>\n      <th>patient_ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249296</th>\n      <td>0.312098</td>\n      <td>0.324304</td>\n      <td>0.385159</td>\n      <td>0.551446</td>\n      <td>0.107906</td>\n      <td>0.385246</td>\n      <td>0.373817</td>\n      <td>0.502808</td>\n      <td>0.272663</td>\n      <td>0.768693</td>\n      <td>...</td>\n      <td>0.582445</td>\n      <td>0.768956</td>\n      <td>0.144002</td>\n      <td>0.659206</td>\n      <td>0.502714</td>\n      <td>0.802432</td>\n      <td>0.059775</td>\n      <td>0.797122</td>\n      <td>0.130086</td>\n      <td>0.176577</td>\n    </tr>\n    <tr>\n      <th>249524</th>\n      <td>0.136203</td>\n      <td>0.208200</td>\n      <td>0.463417</td>\n      <td>0.525977</td>\n      <td>1.000000</td>\n      <td>0.207136</td>\n      <td>0.591409</td>\n      <td>0.439236</td>\n      <td>0.289637</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.595341</td>\n      <td>0.711269</td>\n      <td>0.146812</td>\n      <td>0.791981</td>\n      <td>0.524508</td>\n      <td>0.974547</td>\n      <td>0.434257</td>\n      <td>0.816539</td>\n      <td>0.171427</td>\n      <td>0.124036</td>\n    </tr>\n    <tr>\n      <th>249527</th>\n      <td>0.126140</td>\n      <td>0.898438</td>\n      <td>0.141661</td>\n      <td>0.928443</td>\n      <td>0.476218</td>\n      <td>0.695349</td>\n      <td>0.740925</td>\n      <td>0.408495</td>\n      <td>0.893788</td>\n      <td>0.894466</td>\n      <td>...</td>\n      <td>0.737108</td>\n      <td>0.862702</td>\n      <td>0.167977</td>\n      <td>0.834964</td>\n      <td>0.426567</td>\n      <td>0.925897</td>\n      <td>0.209239</td>\n      <td>0.828620</td>\n      <td>0.140970</td>\n      <td>0.199764</td>\n    </tr>\n    <tr>\n      <th>249529</th>\n      <td>0.256343</td>\n      <td>0.763848</td>\n      <td>0.184840</td>\n      <td>0.799118</td>\n      <td>0.798501</td>\n      <td>0.425631</td>\n      <td>0.528707</td>\n      <td>0.434600</td>\n      <td>0.923083</td>\n      <td>0.842613</td>\n      <td>...</td>\n      <td>0.626282</td>\n      <td>0.769154</td>\n      <td>0.140055</td>\n      <td>0.752708</td>\n      <td>0.393765</td>\n      <td>0.867743</td>\n      <td>0.306340</td>\n      <td>0.714705</td>\n      <td>0.112959</td>\n      <td>0.174203</td>\n    </tr>\n    <tr>\n      <th>249530</th>\n      <td>0.471651</td>\n      <td>0.588570</td>\n      <td>0.474179</td>\n      <td>0.625999</td>\n      <td>0.000000</td>\n      <td>0.624659</td>\n      <td>0.439110</td>\n      <td>0.381255</td>\n      <td>0.314408</td>\n      <td>0.634108</td>\n      <td>...</td>\n      <td>0.655757</td>\n      <td>0.879676</td>\n      <td>0.117141</td>\n      <td>0.752675</td>\n      <td>0.521737</td>\n      <td>0.941098</td>\n      <td>0.360808</td>\n      <td>0.852167</td>\n      <td>0.162523</td>\n      <td>0.220044</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>305260</th>\n      <td>0.057840</td>\n      <td>0.221925</td>\n      <td>0.229260</td>\n      <td>0.124437</td>\n      <td>0.323815</td>\n      <td>0.186135</td>\n      <td>0.082081</td>\n      <td>0.480966</td>\n      <td>0.147167</td>\n      <td>0.760505</td>\n      <td>...</td>\n      <td>0.563893</td>\n      <td>0.658029</td>\n      <td>0.193752</td>\n      <td>0.529967</td>\n      <td>0.427364</td>\n      <td>0.639396</td>\n      <td>0.374160</td>\n      <td>0.657968</td>\n      <td>0.492918</td>\n      <td>0.832546</td>\n    </tr>\n    <tr>\n      <th>305261</th>\n      <td>0.196474</td>\n      <td>0.614002</td>\n      <td>0.238328</td>\n      <td>0.580901</td>\n      <td>0.314771</td>\n      <td>0.445792</td>\n      <td>0.404507</td>\n      <td>0.570273</td>\n      <td>0.546146</td>\n      <td>0.589412</td>\n      <td>...</td>\n      <td>0.383422</td>\n      <td>0.095244</td>\n      <td>0.479955</td>\n      <td>0.345655</td>\n      <td>0.462728</td>\n      <td>0.492059</td>\n      <td>0.486635</td>\n      <td>0.463083</td>\n      <td>0.605367</td>\n      <td>0.531432</td>\n    </tr>\n    <tr>\n      <th>305262</th>\n      <td>0.414299</td>\n      <td>0.682621</td>\n      <td>0.193703</td>\n      <td>0.636208</td>\n      <td>0.318074</td>\n      <td>0.296531</td>\n      <td>0.442509</td>\n      <td>0.572413</td>\n      <td>0.624450</td>\n      <td>0.519011</td>\n      <td>...</td>\n      <td>0.387024</td>\n      <td>0.595301</td>\n      <td>0.443945</td>\n      <td>0.634357</td>\n      <td>0.475175</td>\n      <td>0.369180</td>\n      <td>0.589576</td>\n      <td>0.000000</td>\n      <td>0.822313</td>\n      <td>0.296249</td>\n    </tr>\n    <tr>\n      <th>305263</th>\n      <td>0.468658</td>\n      <td>0.283043</td>\n      <td>0.201599</td>\n      <td>0.345914</td>\n      <td>0.318074</td>\n      <td>0.189903</td>\n      <td>0.183957</td>\n      <td>0.655451</td>\n      <td>0.199281</td>\n      <td>0.368795</td>\n      <td>...</td>\n      <td>0.367569</td>\n      <td>0.387289</td>\n      <td>0.541773</td>\n      <td>0.595527</td>\n      <td>0.364533</td>\n      <td>0.336226</td>\n      <td>0.711637</td>\n      <td>0.022291</td>\n      <td>0.851226</td>\n      <td>0.359374</td>\n    </tr>\n    <tr>\n      <th>305264</th>\n      <td>0.445972</td>\n      <td>0.609305</td>\n      <td>0.571654</td>\n      <td>0.587775</td>\n      <td>0.316866</td>\n      <td>0.432453</td>\n      <td>0.330094</td>\n      <td>0.600416</td>\n      <td>0.517167</td>\n      <td>0.621371</td>\n      <td>...</td>\n      <td>0.364670</td>\n      <td>0.453505</td>\n      <td>0.529371</td>\n      <td>0.623269</td>\n      <td>0.400310</td>\n      <td>0.454424</td>\n      <td>0.397859</td>\n      <td>0.129941</td>\n      <td>0.445945</td>\n      <td>0.462979</td>\n    </tr>\n  </tbody>\n</table>\n<p>649 rows × 1766 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_df_path = \"/var/www/datasets/merged-combat15.csv.xz\"\n",
    "state_df_path = \"/var/www/datasets/embedding_vector_state_and_outcome.csv\"\n",
    "tamoxifen_studies = [\"GSE12093\",  \"GSE1379\", \"GSE17705\", \"GSE6577\",  \"GSE9893\"]\n",
    "ge_df = pd.read_csv(ge_df_path, index_col=\"patient_ID\")\n",
    "state_df = pd.read_csv(state_df_path, index_col=\"patient_ID\")\n",
    "tax_trt_df = state_df[state_df[\"series_id\"].isin(tamoxifen_studies)]\n",
    "ge_tamx_df = ge_df.loc[tax_trt_df.index,:]\n",
    "ge_tamx_df = ge_tamx_df.join(state_df[\"posOutcome\"])\n",
    "X_tam_df, y_tamx_df = ge_tamx_df.drop([\"posOutcome\"], axis=1), ge_tamx_df[\"posOutcome\"]\n",
    "X_scaled = preprocess_data(X_tam_df)\n",
    "print(X_scaled.shape)\n",
    "X_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X_mad_train, X_mad_test, y_tamx_train, y_tamx_test = train_test_split(X_scaled, y_tamx_df, test_size=0.3,\n",
    "                                stratify=y_tamx_df, random_state=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[21:43:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:43:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 52.99 seconds.\n",
      "Best Score: 65.342%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.1, 'n_estimators': 120, 'min_child_weight': 1, 'max_depth': 5, 'max_delta_step': 3, 'learning_rate': 0.07, 'gamma': 1, 'colsample_bytree': 0.8}\n",
      "[21:44:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.653420\n",
      "recall_0               0.675000\n",
      "precision_0            0.398865\n",
      "recall_1               0.631841\n",
      "precision_1            0.846086\n",
      "auc                    0.741060\n",
      "specificity            0.675000\n",
      "average_precision_0    0.230410\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.704861\n",
      "recall_0               0.666667\n",
      "precision_0            0.478873\n",
      "recall_1               0.743056\n",
      "precision_1            0.862903\n",
      "auc                    0.739651\n",
      "specificity            0.666667\n",
      "average_precision_0    0.220058\n",
      "dtype: float64\n",
      "\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.1, 'n_estimators': 120, 'min_child_weight': 1, 'max_depth': 5, 'max_delta_step': 3, 'learning_rate': 0.07, 'gamma': 1, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   47.5s finished\n"
     ]
    }
   ],
   "source": [
    "params_acc_all, clf_acc_all, cv_scores_acc_all, test_scores_acc_all = evaluate_ge((X_mad_train, X_mad_test, y_tamx_train, y_tamx_test),rand_scoring=\"balanced_accuracy\", split=False)\n",
    "print(params_acc_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[21:45:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:45:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 50.47 seconds.\n",
      "Best Score: 25.178%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.1, 'n_estimators': 80, 'min_child_weight': 3, 'max_depth': 5, 'max_delta_step': 3, 'learning_rate': 0.07, 'gamma': 2, 'colsample_bytree': 1.0}\n",
      "[21:45:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.602448\n",
      "recall_0               0.908333\n",
      "precision_0            0.316917\n",
      "recall_1               0.296563\n",
      "precision_1            0.905667\n",
      "auc                    0.695969\n",
      "specificity            0.908333\n",
      "average_precision_0    0.251778\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.616013\n",
      "recall_0               0.843137\n",
      "precision_0            0.328244\n",
      "recall_1               0.388889\n",
      "precision_1            0.875000\n",
      "auc                    0.727669\n",
      "specificity            0.843137\n",
      "average_precision_0    0.240121\n",
      "dtype: float64\n",
      "\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.1, 'n_estimators': 80, 'min_child_weight': 3, 'max_depth': 5, 'max_delta_step': 3, 'learning_rate': 0.07, 'gamma': 2, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   47.6s finished\n"
     ]
    }
   ],
   "source": [
    "params_ap_all, clf_ap_all, cv_scores_ap_all, test_scores_ap_all = evaluate_ge((X_mad_train, X_mad_test, y_tamx_train, y_tamx_test),rand_scoring=average_precision_0, split=False)\n",
    "print(params_ap_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[21:46:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[21:46:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 54.35 seconds.\n",
      "Best Score: 74.515%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.5, 'n_estimators': 120, 'min_child_weight': 1, 'max_depth': 6, 'max_delta_step': 3, 'learning_rate': 0.05, 'gamma': 0.5, 'colsample_bytree': 0.8}\n",
      "[21:46:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.604953\n",
      "recall_0               0.266667\n",
      "precision_0            0.639814\n",
      "recall_1               0.943238\n",
      "precision_1            0.782287\n",
      "auc                    0.745153\n",
      "specificity            0.266667\n",
      "average_precision_0    0.231714\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.544935\n",
      "recall_0               0.117647\n",
      "precision_0            0.600000\n",
      "recall_1               0.972222\n",
      "precision_1            0.756757\n",
      "auc                    0.728894\n",
      "specificity            0.117647\n",
      "average_precision_0    0.245396\n",
      "dtype: float64\n",
      "\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.5, 'n_estimators': 120, 'min_child_weight': 1, 'max_depth': 6, 'max_delta_step': 3, 'learning_rate': 0.05, 'gamma': 0.5, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   47.2s finished\n"
     ]
    }
   ],
   "source": [
    "params_auc_all, clf_auc_all, cv_scores_auc_all, test_scores_auc_all = evaluate_ge((X_mad_train, X_mad_test, y_tamx_train, y_tamx_test), split=False)\n",
    "print(params_auc_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solutions = mrmr.mrmr_ensemble(features=X_mad_train, targets=y_tamx_train.to_frame(), solution_length=100, solution_count=1)\n",
    "feats_100 = solutions[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}