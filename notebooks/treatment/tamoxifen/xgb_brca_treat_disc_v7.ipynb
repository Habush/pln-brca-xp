{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xabush/pln-brca-xp/notebooks\n"
     ]
    }
   ],
   "source": [
    "#In this notebook :\n",
    "#1. Check the effect using different normalization techniques\n",
    "%cd \"~/pln-brca-xp/notebooks\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ge_df = pd.read_csv(\"/var/www/datasets/merged-combat15.csv.xz\", index_col=\"patient_ID\")\n",
    "state_df = pd.read_csv(\"/var/www/datasets/embedding_vector_state_and_outcome.csv\", index_col=\"patient_ID\")\n",
    "ge_df = ge_df.join(state_df[\"posOutcome\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_set(ge_df, \"/var/www/datasets/tamox_train_set.txt\", \"/var/www/datasets/tamox_test_set.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "        MAGEA12_overexpr  MAGEA11_overexpr  KLF1_overexpr  ADH7_overexpr  \\\n249540          0.000000          0.000000       0.000000       0.000000   \n441885          0.757387          1.000000       0.000000       0.837795   \n441672          0.000000          0.000000       0.000000       0.667331   \n441746          0.000000          0.000000       0.000000       0.000000   \n305151          0.000000          0.502497       0.000000       0.000000   \n...                  ...               ...            ...            ...   \n441908          0.000000          0.000000       0.000000       0.577685   \n441675          0.898199          0.000000       0.000000       0.667331   \n441765          0.000000          0.000000       0.000000       0.000000   \n305133          0.000000          0.502497       0.764303       0.000000   \n305238          0.000000          0.502497       0.000000       0.697610   \n\n        MSH4_overexpr  BIRC3_overexpr  AKR1C4_overexpr  GBX2_overexpr  \\\n249540       0.000000        0.712207          0.76379       0.756508   \n441885       0.000000        0.000000          0.00000       0.750000   \n441672       0.750000        0.647551          0.75000       0.000000   \n441746       0.000000        0.000000          0.00000       0.750000   \n305151       0.000000        0.636346          0.00000       0.000000   \n...               ...             ...              ...            ...   \n441908       0.000000        0.000000          0.00000       0.750000   \n441675       0.750000        0.000000          0.75000       0.000000   \n441765       0.000000        0.000000          0.00000       0.750000   \n305133       0.000000        0.000000          0.00000       0.000000   \n305238       0.751134        0.000000          0.00000       0.000000   \n\n        GCGR_overexpr  SIGLEC9_overexpr  ...  ZNF80_underexpr  \\\n249540           0.00          0.000000  ...          0.97893   \n441885           0.75          0.000000  ...          0.00000   \n441672           0.00          0.000000  ...          0.00000   \n441746           0.75          0.000000  ...          0.00000   \n305151           0.00          0.503539  ...          0.75000   \n...               ...               ...  ...              ...   \n441908           0.75          0.000000  ...          0.00000   \n441675           0.00          0.818226  ...          0.00000   \n441765           0.75          0.000000  ...          0.00000   \n305133           0.00          0.503539  ...          0.75000   \n305238           0.00          0.503539  ...          0.75000   \n\n        ZNF83_underexpr  ZNF84_underexpr  ZNF91_underexpr  ZNHIT2_underexpr  \\\n249540         0.000000         0.787505         0.000000              0.00   \n441885         0.000000         0.670366         0.000000              0.00   \n441672         0.541549         0.000000         0.730075              0.00   \n441746         0.000000         0.000000         0.000000              0.00   \n305151         0.558113         0.763324         0.757755              0.00   \n...                 ...              ...              ...               ...   \n441908         0.689546         0.835837         0.000000              0.00   \n441675         0.761012         0.828643         0.822902              0.00   \n441765         0.000000         0.506010         0.601826              0.00   \n305133         0.786319         0.754317         0.770470              0.00   \n305238         0.538691         0.777592         0.573594              0.75   \n\n        ZSCAN2_underexpr  ZXDC_underexpr  ZYX_underexpr  ZZEF1_underexpr  \\\n249540          0.000000        0.506928       0.000000         0.839701   \n441885          0.000000        0.000000       0.000000         0.689889   \n441672          0.628329        0.519482       0.000000         0.000000   \n441746          0.000000        0.000000       0.779612         0.782534   \n305151          0.750000        0.000000       0.000000         0.000000   \n...                  ...             ...            ...              ...   \n441908          0.000000        0.646032       0.000000         0.000000   \n441675          0.628329        0.000000       0.755952         0.000000   \n441765          0.000000        0.000000       0.000000         0.000000   \n305133          0.000000        0.000000       0.000000         0.516633   \n305238          0.750000        0.000000       0.000000         0.795823   \n\n        ZZZ3_underexpr  \n249540        0.660961  \n441885        0.000000  \n441672        0.759948  \n441746        0.000000  \n305151        0.744926  \n...                ...  \n441908        0.000000  \n441675        0.782249  \n441765        0.000000  \n305133        0.791179  \n305238        0.000000  \n\n[193 rows x 17664 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAGEA12_overexpr</th>\n      <th>MAGEA11_overexpr</th>\n      <th>KLF1_overexpr</th>\n      <th>ADH7_overexpr</th>\n      <th>MSH4_overexpr</th>\n      <th>BIRC3_overexpr</th>\n      <th>AKR1C4_overexpr</th>\n      <th>GBX2_overexpr</th>\n      <th>GCGR_overexpr</th>\n      <th>SIGLEC9_overexpr</th>\n      <th>...</th>\n      <th>ZNF80_underexpr</th>\n      <th>ZNF83_underexpr</th>\n      <th>ZNF84_underexpr</th>\n      <th>ZNF91_underexpr</th>\n      <th>ZNHIT2_underexpr</th>\n      <th>ZSCAN2_underexpr</th>\n      <th>ZXDC_underexpr</th>\n      <th>ZYX_underexpr</th>\n      <th>ZZEF1_underexpr</th>\n      <th>ZZZ3_underexpr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249540</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.712207</td>\n      <td>0.76379</td>\n      <td>0.756508</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.97893</td>\n      <td>0.000000</td>\n      <td>0.787505</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.506928</td>\n      <td>0.000000</td>\n      <td>0.839701</td>\n      <td>0.660961</td>\n    </tr>\n    <tr>\n      <th>441885</th>\n      <td>0.757387</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.837795</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.750000</td>\n      <td>0.75</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.670366</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.689889</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>441672</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.667331</td>\n      <td>0.750000</td>\n      <td>0.647551</td>\n      <td>0.75000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.541549</td>\n      <td>0.000000</td>\n      <td>0.730075</td>\n      <td>0.00</td>\n      <td>0.628329</td>\n      <td>0.519482</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.759948</td>\n    </tr>\n    <tr>\n      <th>441746</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.750000</td>\n      <td>0.75</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.779612</td>\n      <td>0.782534</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>305151</th>\n      <td>0.000000</td>\n      <td>0.502497</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.636346</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.503539</td>\n      <td>...</td>\n      <td>0.75000</td>\n      <td>0.558113</td>\n      <td>0.763324</td>\n      <td>0.757755</td>\n      <td>0.00</td>\n      <td>0.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.744926</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>441908</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.577685</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.750000</td>\n      <td>0.75</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.689546</td>\n      <td>0.835837</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.646032</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>441675</th>\n      <td>0.898199</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.667331</td>\n      <td>0.750000</td>\n      <td>0.000000</td>\n      <td>0.75000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.818226</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.761012</td>\n      <td>0.828643</td>\n      <td>0.822902</td>\n      <td>0.00</td>\n      <td>0.628329</td>\n      <td>0.000000</td>\n      <td>0.755952</td>\n      <td>0.000000</td>\n      <td>0.782249</td>\n    </tr>\n    <tr>\n      <th>441765</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.750000</td>\n      <td>0.75</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.506010</td>\n      <td>0.601826</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>305133</th>\n      <td>0.000000</td>\n      <td>0.502497</td>\n      <td>0.764303</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.503539</td>\n      <td>...</td>\n      <td>0.75000</td>\n      <td>0.786319</td>\n      <td>0.754317</td>\n      <td>0.770470</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.516633</td>\n      <td>0.791179</td>\n    </tr>\n    <tr>\n      <th>305238</th>\n      <td>0.000000</td>\n      <td>0.502497</td>\n      <td>0.000000</td>\n      <td>0.697610</td>\n      <td>0.751134</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.503539</td>\n      <td>...</td>\n      <td>0.75000</td>\n      <td>0.538691</td>\n      <td>0.777592</td>\n      <td>0.573594</td>\n      <td>0.75</td>\n      <td>0.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.795823</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows × 17664 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize to uniform distribtion\n",
    "uni_qnorm = MQNormalizer(subsample=X_train.shape[0])\n",
    "X_train_uni_q = uni_qnorm.fit_transform(X_train)\n",
    "X_test_uni_q = uni_qnorm.transform(X_test)\n",
    "X_test_uni_q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "g_qnorm = MQNormalizer(scaler=QuantileTransformer(n_quantiles=5, subsample=X_train.shape[0], output_distribution=\"normal\"))\n",
    "X_train_g_norm_q = g_qnorm.fit_transform(X_train)\n",
    "X_test_g_norm_q = g_qnorm.transform(X_test)\n",
    "X_train_g_norm_q.to_csv(\"/var/www/datasets/ge_out_tamoxifen_qnorm_gaussian_train.csv\", index_label=True)\n",
    "X_test_g_norm_q.to_csv(\"/var/www/datasets/ge_out_tamoxifen_qnorm_gaussian_test.csv\", index_label=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "        MAGEA12_overexpr  MAGEA11_overexpr  KLF1_overexpr  ADH7_overexpr  \\\n22473           0.021280          0.000000       0.095147       0.000000   \n249608          0.099072          0.086824       0.107329       0.000000   \n249599          0.163417          0.143724       0.043707       0.031337   \n441791          0.000000          0.000000       0.000000       0.000000   \n249560          0.018554          0.000000       0.024099       0.000000   \n...                  ...               ...            ...            ...   \n22502           0.059847          0.003792       0.106235       0.060974   \n441685          0.000000          0.000000       0.114740       0.028305   \n441899          0.000000          0.000000       0.000000       0.561149   \n305219          0.000000          0.000262       0.000000       0.000000   \n249618          0.000000          0.063628       0.000000       0.129430   \n\n        MSH4_overexpr  BIRC3_overexpr  AKR1C4_overexpr  GBX2_overexpr  \\\n22473        0.044741        0.233028         0.088429       0.133873   \n249608       0.151601        0.000000         0.041309       0.161798   \n249599       0.142185        0.067910         0.000000       0.000000   \n441791       0.000000        0.000000         0.000000       0.008921   \n249560       0.000000        0.000000         0.000000       0.072115   \n...               ...             ...              ...            ...   \n22502        0.000000        0.015950         0.011834       0.000000   \n441685       0.005041        0.047882         0.005042       0.000000   \n441899       0.000000        0.000000         0.000000       0.008921   \n305219       0.000000        0.251733         0.000000       0.000000   \n249618       0.000000        0.000000         0.000000       0.000000   \n\n        GCGR_overexpr  SIGLEC9_overexpr  ...  ZNF80_underexpr  \\\n22473        0.005640          0.050008  ...         0.000000   \n249608       0.027474          0.094637  ...         0.000000   \n249599       0.000000          0.085517  ...         0.126907   \n441791       0.000865          0.000000  ...         0.000000   \n249560       0.063771          0.126233  ...         0.000000   \n...               ...               ...  ...              ...   \n22502        0.000000          0.014699  ...         0.027960   \n441685       0.000000          0.000000  ...         0.000000   \n441899       0.000865          0.000000  ...         0.000000   \n305219       0.000000          0.000696  ...         0.026176   \n249618       0.116817          0.008612  ...         0.161610   \n\n        ZNF83_underexpr  ZNF84_underexpr  ZNF91_underexpr  ZNHIT2_underexpr  \\\n22473          0.327463         0.472138         0.241346          0.282492   \n249608         0.000000         0.000000         0.031138          0.000000   \n249599         0.000000         0.000000         0.011940          0.379616   \n441791         0.000000         0.307221         0.009090          0.000000   \n249560         0.326511         0.099769         0.198067          0.000000   \n...                 ...              ...              ...               ...   \n22502          0.000000         0.000000         0.000000          0.019576   \n441685         0.230397         0.634768         0.326745          0.000000   \n441899         0.000000         0.000000         0.000000          0.000000   \n305219         0.000000         0.120439         0.000000          0.109655   \n249618         0.000000         0.000000         0.048316          0.282283   \n\n        ZSCAN2_underexpr  ZXDC_underexpr  ZYX_underexpr  ZZEF1_underexpr  \\\n22473           0.000000        0.000000       0.000000         0.000000   \n249608          0.000000        0.000000       0.261074         0.000000   \n249599          0.080480        0.000000       0.062039         0.000000   \n441791          0.000000        0.000000       0.031169         0.192291   \n249560          0.100177        0.000000       0.055225         0.000000   \n...                  ...             ...            ...              ...   \n22502           0.000000        0.000000       0.000000         0.288401   \n441685          0.000000        0.000000       0.232659         0.000000   \n441899          0.000000        0.000000       0.000000         0.000000   \n305219          0.101706        0.174992       0.000000         0.287040   \n249618          0.151061        0.033509       0.000000         0.086494   \n\n        ZZZ3_underexpr  \n22473         0.444162  \n249608        0.272342  \n249599        0.056920  \n441791        0.000000  \n249560        0.500054  \n...                ...  \n22502         0.000000  \n441685        0.000000  \n441899        0.314837  \n305219        0.000000  \n249618        0.000000  \n\n[449 rows x 17664 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAGEA12_overexpr</th>\n      <th>MAGEA11_overexpr</th>\n      <th>KLF1_overexpr</th>\n      <th>ADH7_overexpr</th>\n      <th>MSH4_overexpr</th>\n      <th>BIRC3_overexpr</th>\n      <th>AKR1C4_overexpr</th>\n      <th>GBX2_overexpr</th>\n      <th>GCGR_overexpr</th>\n      <th>SIGLEC9_overexpr</th>\n      <th>...</th>\n      <th>ZNF80_underexpr</th>\n      <th>ZNF83_underexpr</th>\n      <th>ZNF84_underexpr</th>\n      <th>ZNF91_underexpr</th>\n      <th>ZNHIT2_underexpr</th>\n      <th>ZSCAN2_underexpr</th>\n      <th>ZXDC_underexpr</th>\n      <th>ZYX_underexpr</th>\n      <th>ZZEF1_underexpr</th>\n      <th>ZZZ3_underexpr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22473</th>\n      <td>0.021280</td>\n      <td>0.000000</td>\n      <td>0.095147</td>\n      <td>0.000000</td>\n      <td>0.044741</td>\n      <td>0.233028</td>\n      <td>0.088429</td>\n      <td>0.133873</td>\n      <td>0.005640</td>\n      <td>0.050008</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.327463</td>\n      <td>0.472138</td>\n      <td>0.241346</td>\n      <td>0.282492</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.444162</td>\n    </tr>\n    <tr>\n      <th>249608</th>\n      <td>0.099072</td>\n      <td>0.086824</td>\n      <td>0.107329</td>\n      <td>0.000000</td>\n      <td>0.151601</td>\n      <td>0.000000</td>\n      <td>0.041309</td>\n      <td>0.161798</td>\n      <td>0.027474</td>\n      <td>0.094637</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.031138</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.261074</td>\n      <td>0.000000</td>\n      <td>0.272342</td>\n    </tr>\n    <tr>\n      <th>249599</th>\n      <td>0.163417</td>\n      <td>0.143724</td>\n      <td>0.043707</td>\n      <td>0.031337</td>\n      <td>0.142185</td>\n      <td>0.067910</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.085517</td>\n      <td>...</td>\n      <td>0.126907</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.011940</td>\n      <td>0.379616</td>\n      <td>0.080480</td>\n      <td>0.000000</td>\n      <td>0.062039</td>\n      <td>0.000000</td>\n      <td>0.056920</td>\n    </tr>\n    <tr>\n      <th>441791</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008921</td>\n      <td>0.000865</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.307221</td>\n      <td>0.009090</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.031169</td>\n      <td>0.192291</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>249560</th>\n      <td>0.018554</td>\n      <td>0.000000</td>\n      <td>0.024099</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.072115</td>\n      <td>0.063771</td>\n      <td>0.126233</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.326511</td>\n      <td>0.099769</td>\n      <td>0.198067</td>\n      <td>0.000000</td>\n      <td>0.100177</td>\n      <td>0.000000</td>\n      <td>0.055225</td>\n      <td>0.000000</td>\n      <td>0.500054</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22502</th>\n      <td>0.059847</td>\n      <td>0.003792</td>\n      <td>0.106235</td>\n      <td>0.060974</td>\n      <td>0.000000</td>\n      <td>0.015950</td>\n      <td>0.011834</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.014699</td>\n      <td>...</td>\n      <td>0.027960</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.019576</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.288401</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>441685</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.114740</td>\n      <td>0.028305</td>\n      <td>0.005041</td>\n      <td>0.047882</td>\n      <td>0.005042</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.230397</td>\n      <td>0.634768</td>\n      <td>0.326745</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.232659</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>441899</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.561149</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008921</td>\n      <td>0.000865</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.314837</td>\n    </tr>\n    <tr>\n      <th>305219</th>\n      <td>0.000000</td>\n      <td>0.000262</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.251733</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000696</td>\n      <td>...</td>\n      <td>0.026176</td>\n      <td>0.000000</td>\n      <td>0.120439</td>\n      <td>0.000000</td>\n      <td>0.109655</td>\n      <td>0.101706</td>\n      <td>0.174992</td>\n      <td>0.000000</td>\n      <td>0.287040</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>249618</th>\n      <td>0.000000</td>\n      <td>0.063628</td>\n      <td>0.000000</td>\n      <td>0.129430</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.116817</td>\n      <td>0.008612</td>\n      <td>...</td>\n      <td>0.161610</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.048316</td>\n      <td>0.282283</td>\n      <td>0.151061</td>\n      <td>0.033509</td>\n      <td>0.000000</td>\n      <td>0.086494</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>449 rows × 17664 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "ssg_qnorm = MQNormalizer(scaler=MinMaxScaler())\n",
    "X_train_g_norm_ssq = ssg_qnorm.fit_transform(X_train)\n",
    "X_test_g_norm_ssq = ssg_qnorm.transform(X_test)\n",
    "X_train_g_norm_ssq.to_csv(\"/var/www/datasets/ge_out_tamoxifen_qnorm_gaussian_ss_train.csv\", index_label=True)\n",
    "X_test_g_norm_ssq.to_csv(\"/var/www/datasets/ge_out_tamoxifen_qnorm_gaussian_ss_test.csv\", index_label=True)\n",
    "X_train_g_norm_ssq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "        MAGEA12_overexpr  MAGEA11_overexpr  KLF1_overexpr  ADH7_overexpr  \\\n249540         -5.199338         -5.199338      -5.199338      -5.199338   \n441885          0.697922          5.199338      -5.199338       0.985435   \n441672         -5.199338         -5.199338      -5.199338       0.432556   \n441746         -5.199338         -5.199338      -5.199338      -5.199338   \n305151         -5.199338          0.006259      -5.199338      -5.199338   \n...                  ...               ...            ...            ...   \n441908         -5.199338         -5.199338      -5.199338       0.195975   \n441675          1.271358         -5.199338      -5.199338       0.432556   \n441765         -5.199338         -5.199338      -5.199338      -5.199338   \n305133         -5.199338          0.006259       0.720212      -5.199338   \n305238         -5.199338          0.006259      -5.199338       0.517539   \n\n        MSH4_overexpr  BIRC3_overexpr  AKR1C4_overexpr  GBX2_overexpr  \\\n249540      -5.199338        0.559842         0.718548       0.695114   \n441885      -5.199338       -5.199338        -5.199338       0.674490   \n441672       0.674490        0.378717         0.674490      -5.199338   \n441746      -5.199338       -5.199338        -5.199338       0.674490   \n305151      -5.199338        0.348707        -5.199338      -5.199338   \n...               ...             ...              ...            ...   \n441908      -5.199338       -5.199338        -5.199338       0.674490   \n441675       0.674490       -5.199338         0.674490      -5.199338   \n441765      -5.199338       -5.199338        -5.199338       0.674490   \n305133      -5.199338       -5.199338        -5.199338      -5.199338   \n305238       0.678063       -5.199338        -5.199338      -5.199338   \n\n        GCGR_overexpr  SIGLEC9_overexpr  ...  ZNF83_underexpr  \\\n249540      -5.199338         -5.199338  ...        -5.199338   \n441885       0.674490         -5.199338  ...        -5.199338   \n441672      -5.199338         -5.199338  ...         0.104338   \n441746       0.674490         -5.199338  ...        -5.199338   \n305151      -5.199338          0.008870  ...         0.146186   \n...               ...               ...  ...              ...   \n441908       0.674490         -5.199338  ...         0.494563   \n441675      -5.199338          0.908625  ...         0.709562   \n441765       0.674490         -5.199338  ...        -5.199338   \n305133      -5.199338          0.008870  ...         0.793715   \n305238      -5.199338          0.008870  ...         0.097137   \n\n        ZNF84_underexpr  ZNF91_underexpr  ZNHIT2_underexpr  ZSCAN2_underexpr  \\\n249540         0.797795        -5.199338         -5.199338         -5.199338   \n441885         0.440925        -5.199338         -5.199338         -5.199338   \n441672        -5.199338         0.613041         -5.199338          0.327430   \n441746        -5.199338        -5.199338         -5.199338         -5.199338   \n305151         0.717035         0.699100         -5.199338          0.674490   \n...                 ...              ...               ...               ...   \n441908         0.977493        -5.199338         -5.199338         -5.199338   \n441675         0.948818         0.926481         -5.199338          0.327430   \n441765         0.015065         0.258076         -5.199338         -5.199338   \n305133         0.688137         0.740396         -5.199338         -5.199338   \n305238         0.764085         0.185531          0.674490          0.674490   \n\n        ZXDC_underexpr  ZYX_underexpr  ZZEF1_underexpr  ZZZ3_underexpr  \\\n249540        0.017368      -5.199338         0.993229        0.415087   \n441885       -5.199338      -5.199338         0.495536       -5.199338   \n441672        0.048853      -5.199338        -5.199338        0.706135   \n441746       -5.199338       0.770884         0.780779       -5.199338   \n305151       -5.199338      -5.199338        -5.199338        0.658609   \n...                ...            ...              ...             ...   \n441908        0.374631      -5.199338        -5.199338       -5.199338   \n441675       -5.199338       0.693339        -5.199338        0.779813   \n441765       -5.199338      -5.199338        -5.199338       -5.199338   \n305133       -5.199338      -5.199338         0.041705        0.810519   \n305238       -5.199338      -5.199338         0.826792       -5.199338   \n\n        posOutcome  \n249540           0  \n441885           0  \n441672           0  \n441746           0  \n305151           0  \n...            ...  \n441908           0  \n441675           1  \n441765           1  \n305133           1  \n305238           1  \n\n[193 rows x 17665 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAGEA12_overexpr</th>\n      <th>MAGEA11_overexpr</th>\n      <th>KLF1_overexpr</th>\n      <th>ADH7_overexpr</th>\n      <th>MSH4_overexpr</th>\n      <th>BIRC3_overexpr</th>\n      <th>AKR1C4_overexpr</th>\n      <th>GBX2_overexpr</th>\n      <th>GCGR_overexpr</th>\n      <th>SIGLEC9_overexpr</th>\n      <th>...</th>\n      <th>ZNF83_underexpr</th>\n      <th>ZNF84_underexpr</th>\n      <th>ZNF91_underexpr</th>\n      <th>ZNHIT2_underexpr</th>\n      <th>ZSCAN2_underexpr</th>\n      <th>ZXDC_underexpr</th>\n      <th>ZYX_underexpr</th>\n      <th>ZZEF1_underexpr</th>\n      <th>ZZZ3_underexpr</th>\n      <th>posOutcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249540</th>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.559842</td>\n      <td>0.718548</td>\n      <td>0.695114</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>...</td>\n      <td>-5.199338</td>\n      <td>0.797795</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.017368</td>\n      <td>-5.199338</td>\n      <td>0.993229</td>\n      <td>0.415087</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>441885</th>\n      <td>0.697922</td>\n      <td>5.199338</td>\n      <td>-5.199338</td>\n      <td>0.985435</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.674490</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>...</td>\n      <td>-5.199338</td>\n      <td>0.440925</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.495536</td>\n      <td>-5.199338</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>441672</th>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.432556</td>\n      <td>0.674490</td>\n      <td>0.378717</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>...</td>\n      <td>0.104338</td>\n      <td>-5.199338</td>\n      <td>0.613041</td>\n      <td>-5.199338</td>\n      <td>0.327430</td>\n      <td>0.048853</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.706135</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>441746</th>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.674490</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>...</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.770884</td>\n      <td>0.780779</td>\n      <td>-5.199338</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>305151</th>\n      <td>-5.199338</td>\n      <td>0.006259</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.348707</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.008870</td>\n      <td>...</td>\n      <td>0.146186</td>\n      <td>0.717035</td>\n      <td>0.699100</td>\n      <td>-5.199338</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.658609</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>441908</th>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.195975</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.674490</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>...</td>\n      <td>0.494563</td>\n      <td>0.977493</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.374631</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>441675</th>\n      <td>1.271358</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.432556</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.908625</td>\n      <td>...</td>\n      <td>0.709562</td>\n      <td>0.948818</td>\n      <td>0.926481</td>\n      <td>-5.199338</td>\n      <td>0.327430</td>\n      <td>-5.199338</td>\n      <td>0.693339</td>\n      <td>-5.199338</td>\n      <td>0.779813</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>441765</th>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.674490</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>...</td>\n      <td>-5.199338</td>\n      <td>0.015065</td>\n      <td>0.258076</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>305133</th>\n      <td>-5.199338</td>\n      <td>0.006259</td>\n      <td>0.720212</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.008870</td>\n      <td>...</td>\n      <td>0.793715</td>\n      <td>0.688137</td>\n      <td>0.740396</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.041705</td>\n      <td>0.810519</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>305238</th>\n      <td>-5.199338</td>\n      <td>0.006259</td>\n      <td>-5.199338</td>\n      <td>0.517539</td>\n      <td>0.678063</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.008870</td>\n      <td>...</td>\n      <td>0.097137</td>\n      <td>0.764085</td>\n      <td>0.185531</td>\n      <td>0.674490</td>\n      <td>0.674490</td>\n      <td>-5.199338</td>\n      <td>-5.199338</td>\n      <td>0.826792</td>\n      <td>-5.199338</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows × 17665 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_g_norm_q_out = X_train_g_norm_q.join(ge_df[\"posOutcome\"])\n",
    "X_test_g_norm_q_out = X_test_g_norm_q.join(ge_df[\"posOutcome\"])\n",
    "X_test_g_norm_q_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "         MAGEA12   MAGEA11      KLF1      ADH7      MSH4     BIRC3    AKR1C4  \\\n249540 -0.012612 -0.033102  0.064470 -0.035743  0.046418  0.332813  0.187615   \n441885  0.152488  1.444133  0.207721  0.522381  0.100024  0.235817  0.135829   \n441672  0.117885  0.322986  0.203585  0.253117  0.105531  0.306808  0.140186   \n441746  0.126683  0.316034  0.207721  0.215761  0.100024  0.235817  0.135829   \n305151  0.123684  0.323163  0.198217  0.231361  0.100999  0.302301  0.135328   \n...          ...       ...       ...       ...       ...       ...       ...   \n441908  0.126683  0.316034  0.207721  0.241461  0.100024  0.230097  0.135829   \n441675  0.644383  0.322986  0.203585  0.253117  0.105531  0.159497  0.140186   \n441765  0.126683  0.316034  0.207721  0.215768  0.100024  0.235817  0.135829   \n305133  0.123684  0.323163  0.260336  0.231361  0.100999  0.247462  0.135328   \n305238  0.123684  0.323163  0.198217  0.257054  0.109589  0.247462  0.135328   \n\n            GBX2      GCGR   SIGLEC9  ...     ZNF80     ZNF83     ZNF84  \\\n249540  0.202148  0.148351  0.048806  ...  0.023543  0.619112  0.457458   \n441885  0.180823  0.157440  0.262362  ...  0.286841  0.858773  0.569379   \n441672  0.170860  0.156711  0.256002  ...  0.297475  0.561261  0.713150   \n441746  0.180823  0.157440  0.262362  ...  0.286841  0.668119  0.753780   \n305151  0.173449  0.155482  0.262875  ...  0.279333  0.553355  0.509516   \n...          ...       ...       ...  ...       ...       ...       ...   \n441908  0.180823  0.157440  0.262362  ...  0.286841  0.490617  0.353409   \n441675  0.170860  0.156711  0.490022  ...  0.297475  0.441419  0.368896   \n441765  0.180823  0.157440  0.262362  ...  0.286841  0.703829  0.633731   \n305133  0.173449  0.155482  0.262875  ...  0.279333  0.394676  0.528906   \n305238  0.173449  0.155482  0.262875  ...  0.279333  0.562625  0.478800   \n\n           ZNF91    ZNHIT2    ZSCAN2      ZXDC       ZYX     ZZEF1      ZZZ3  \n249540  0.812111  0.452773  0.315088  0.449364  0.778671  0.264786  0.415394  \n441885  0.775311  0.244023  0.258659  0.686643  0.479418  0.435768  0.546380  \n441672  0.557539  0.285374  0.245156  0.445551  0.624766  0.699554  0.363864  \n441746  0.646312  0.244023  0.258659  0.596188  0.364117  0.359216  0.623518  \n305151  0.535441  0.244044  0.232352  0.489395  0.479349  0.679984  0.381019  \n...          ...       ...       ...       ...       ...       ...       ...  \n441908  0.665294  0.244023  0.258659  0.407119  0.540433  0.512047  0.593405  \n441675  0.391445  0.288958  0.245156  0.454411  0.403208  0.701180  0.330060  \n441765  0.589447  0.244023  0.258659  0.469656  0.496578  0.570634  0.619272  \n305133  0.507336  0.285471  0.298997  0.662130  0.609503  0.501518  0.316524  \n305238  0.596472  0.224087  0.232352  0.483663  0.499843  0.337266  0.562459  \n\n[193 rows x 8832 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAGEA12</th>\n      <th>MAGEA11</th>\n      <th>KLF1</th>\n      <th>ADH7</th>\n      <th>MSH4</th>\n      <th>BIRC3</th>\n      <th>AKR1C4</th>\n      <th>GBX2</th>\n      <th>GCGR</th>\n      <th>SIGLEC9</th>\n      <th>...</th>\n      <th>ZNF80</th>\n      <th>ZNF83</th>\n      <th>ZNF84</th>\n      <th>ZNF91</th>\n      <th>ZNHIT2</th>\n      <th>ZSCAN2</th>\n      <th>ZXDC</th>\n      <th>ZYX</th>\n      <th>ZZEF1</th>\n      <th>ZZZ3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249540</th>\n      <td>-0.012612</td>\n      <td>-0.033102</td>\n      <td>0.064470</td>\n      <td>-0.035743</td>\n      <td>0.046418</td>\n      <td>0.332813</td>\n      <td>0.187615</td>\n      <td>0.202148</td>\n      <td>0.148351</td>\n      <td>0.048806</td>\n      <td>...</td>\n      <td>0.023543</td>\n      <td>0.619112</td>\n      <td>0.457458</td>\n      <td>0.812111</td>\n      <td>0.452773</td>\n      <td>0.315088</td>\n      <td>0.449364</td>\n      <td>0.778671</td>\n      <td>0.264786</td>\n      <td>0.415394</td>\n    </tr>\n    <tr>\n      <th>441885</th>\n      <td>0.152488</td>\n      <td>1.444133</td>\n      <td>0.207721</td>\n      <td>0.522381</td>\n      <td>0.100024</td>\n      <td>0.235817</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.858773</td>\n      <td>0.569379</td>\n      <td>0.775311</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.686643</td>\n      <td>0.479418</td>\n      <td>0.435768</td>\n      <td>0.546380</td>\n    </tr>\n    <tr>\n      <th>441672</th>\n      <td>0.117885</td>\n      <td>0.322986</td>\n      <td>0.203585</td>\n      <td>0.253117</td>\n      <td>0.105531</td>\n      <td>0.306808</td>\n      <td>0.140186</td>\n      <td>0.170860</td>\n      <td>0.156711</td>\n      <td>0.256002</td>\n      <td>...</td>\n      <td>0.297475</td>\n      <td>0.561261</td>\n      <td>0.713150</td>\n      <td>0.557539</td>\n      <td>0.285374</td>\n      <td>0.245156</td>\n      <td>0.445551</td>\n      <td>0.624766</td>\n      <td>0.699554</td>\n      <td>0.363864</td>\n    </tr>\n    <tr>\n      <th>441746</th>\n      <td>0.126683</td>\n      <td>0.316034</td>\n      <td>0.207721</td>\n      <td>0.215761</td>\n      <td>0.100024</td>\n      <td>0.235817</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.668119</td>\n      <td>0.753780</td>\n      <td>0.646312</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.596188</td>\n      <td>0.364117</td>\n      <td>0.359216</td>\n      <td>0.623518</td>\n    </tr>\n    <tr>\n      <th>305151</th>\n      <td>0.123684</td>\n      <td>0.323163</td>\n      <td>0.198217</td>\n      <td>0.231361</td>\n      <td>0.100999</td>\n      <td>0.302301</td>\n      <td>0.135328</td>\n      <td>0.173449</td>\n      <td>0.155482</td>\n      <td>0.262875</td>\n      <td>...</td>\n      <td>0.279333</td>\n      <td>0.553355</td>\n      <td>0.509516</td>\n      <td>0.535441</td>\n      <td>0.244044</td>\n      <td>0.232352</td>\n      <td>0.489395</td>\n      <td>0.479349</td>\n      <td>0.679984</td>\n      <td>0.381019</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>441908</th>\n      <td>0.126683</td>\n      <td>0.316034</td>\n      <td>0.207721</td>\n      <td>0.241461</td>\n      <td>0.100024</td>\n      <td>0.230097</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.490617</td>\n      <td>0.353409</td>\n      <td>0.665294</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.407119</td>\n      <td>0.540433</td>\n      <td>0.512047</td>\n      <td>0.593405</td>\n    </tr>\n    <tr>\n      <th>441675</th>\n      <td>0.644383</td>\n      <td>0.322986</td>\n      <td>0.203585</td>\n      <td>0.253117</td>\n      <td>0.105531</td>\n      <td>0.159497</td>\n      <td>0.140186</td>\n      <td>0.170860</td>\n      <td>0.156711</td>\n      <td>0.490022</td>\n      <td>...</td>\n      <td>0.297475</td>\n      <td>0.441419</td>\n      <td>0.368896</td>\n      <td>0.391445</td>\n      <td>0.288958</td>\n      <td>0.245156</td>\n      <td>0.454411</td>\n      <td>0.403208</td>\n      <td>0.701180</td>\n      <td>0.330060</td>\n    </tr>\n    <tr>\n      <th>441765</th>\n      <td>0.126683</td>\n      <td>0.316034</td>\n      <td>0.207721</td>\n      <td>0.215768</td>\n      <td>0.100024</td>\n      <td>0.235817</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.703829</td>\n      <td>0.633731</td>\n      <td>0.589447</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.469656</td>\n      <td>0.496578</td>\n      <td>0.570634</td>\n      <td>0.619272</td>\n    </tr>\n    <tr>\n      <th>305133</th>\n      <td>0.123684</td>\n      <td>0.323163</td>\n      <td>0.260336</td>\n      <td>0.231361</td>\n      <td>0.100999</td>\n      <td>0.247462</td>\n      <td>0.135328</td>\n      <td>0.173449</td>\n      <td>0.155482</td>\n      <td>0.262875</td>\n      <td>...</td>\n      <td>0.279333</td>\n      <td>0.394676</td>\n      <td>0.528906</td>\n      <td>0.507336</td>\n      <td>0.285471</td>\n      <td>0.298997</td>\n      <td>0.662130</td>\n      <td>0.609503</td>\n      <td>0.501518</td>\n      <td>0.316524</td>\n    </tr>\n    <tr>\n      <th>305238</th>\n      <td>0.123684</td>\n      <td>0.323163</td>\n      <td>0.198217</td>\n      <td>0.257054</td>\n      <td>0.109589</td>\n      <td>0.247462</td>\n      <td>0.135328</td>\n      <td>0.173449</td>\n      <td>0.155482</td>\n      <td>0.262875</td>\n      <td>...</td>\n      <td>0.279333</td>\n      <td>0.562625</td>\n      <td>0.478800</td>\n      <td>0.596472</td>\n      <td>0.224087</td>\n      <td>0.232352</td>\n      <td>0.483663</td>\n      <td>0.499843</td>\n      <td>0.337266</td>\n      <td>0.562459</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows × 8832 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_ss_q = ss.fit_transform(X_train)\n",
    "X_train_ss_q = pd.DataFrame(X_train_ss_q, columns=X_train.columns, index=X_train.index)\n",
    "X_test_ss_q = ss.transform(X_test)\n",
    "X_test_ss_q = pd.DataFrame(X_test_ss_q, columns=X_test.columns, index=X_test.index)\n",
    "X_test_ss_q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "         MAGEA12   MAGEA11      KLF1      ADH7      MSH4     BIRC3    AKR1C4  \\\n249540 -0.012612 -0.033102  0.064470 -0.035743  0.046418  0.332813  0.187615   \n441885  0.152488  1.444133  0.207721  0.522381  0.100024  0.235817  0.135829   \n441672  0.117885  0.322986  0.203585  0.253117  0.105531  0.306808  0.140186   \n441746  0.126683  0.316034  0.207721  0.215761  0.100024  0.235817  0.135829   \n305151  0.123684  0.323163  0.198217  0.231361  0.100999  0.302301  0.135328   \n...          ...       ...       ...       ...       ...       ...       ...   \n441908  0.126683  0.316034  0.207721  0.241461  0.100024  0.230097  0.135829   \n441675  0.644383  0.322986  0.203585  0.253117  0.105531  0.159497  0.140186   \n441765  0.126683  0.316034  0.207721  0.215768  0.100024  0.235817  0.135829   \n305133  0.123684  0.323163  0.260336  0.231361  0.100999  0.247462  0.135328   \n305238  0.123684  0.323163  0.198217  0.257054  0.109589  0.247462  0.135328   \n\n            GBX2      GCGR   SIGLEC9  ...     ZNF80     ZNF83     ZNF84  \\\n249540  0.202148  0.148351  0.048806  ...  0.023543  0.619112  0.457458   \n441885  0.180823  0.157440  0.262362  ...  0.286841  0.858773  0.569379   \n441672  0.170860  0.156711  0.256002  ...  0.297475  0.561261  0.713150   \n441746  0.180823  0.157440  0.262362  ...  0.286841  0.668119  0.753780   \n305151  0.173449  0.155482  0.262875  ...  0.279333  0.553355  0.509516   \n...          ...       ...       ...  ...       ...       ...       ...   \n441908  0.180823  0.157440  0.262362  ...  0.286841  0.490617  0.353409   \n441675  0.170860  0.156711  0.490022  ...  0.297475  0.441419  0.368896   \n441765  0.180823  0.157440  0.262362  ...  0.286841  0.703829  0.633731   \n305133  0.173449  0.155482  0.262875  ...  0.279333  0.394676  0.528906   \n305238  0.173449  0.155482  0.262875  ...  0.279333  0.562625  0.478800   \n\n           ZNF91    ZNHIT2    ZSCAN2      ZXDC       ZYX     ZZEF1      ZZZ3  \n249540  0.812111  0.452773  0.315088  0.449364  0.778671  0.264786  0.415394  \n441885  0.775311  0.244023  0.258659  0.686643  0.479418  0.435768  0.546380  \n441672  0.557539  0.285374  0.245156  0.445551  0.624766  0.699554  0.363864  \n441746  0.646312  0.244023  0.258659  0.596188  0.364117  0.359216  0.623518  \n305151  0.535441  0.244044  0.232352  0.489395  0.479349  0.679984  0.381019  \n...          ...       ...       ...       ...       ...       ...       ...  \n441908  0.665294  0.244023  0.258659  0.407119  0.540433  0.512047  0.593405  \n441675  0.391445  0.288958  0.245156  0.454411  0.403208  0.701180  0.330060  \n441765  0.589447  0.244023  0.258659  0.469656  0.496578  0.570634  0.619272  \n305133  0.507336  0.285471  0.298997  0.662130  0.609503  0.501518  0.316524  \n305238  0.596472  0.224087  0.232352  0.483663  0.499843  0.337266  0.562459  \n\n[193 rows x 8832 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAGEA12</th>\n      <th>MAGEA11</th>\n      <th>KLF1</th>\n      <th>ADH7</th>\n      <th>MSH4</th>\n      <th>BIRC3</th>\n      <th>AKR1C4</th>\n      <th>GBX2</th>\n      <th>GCGR</th>\n      <th>SIGLEC9</th>\n      <th>...</th>\n      <th>ZNF80</th>\n      <th>ZNF83</th>\n      <th>ZNF84</th>\n      <th>ZNF91</th>\n      <th>ZNHIT2</th>\n      <th>ZSCAN2</th>\n      <th>ZXDC</th>\n      <th>ZYX</th>\n      <th>ZZEF1</th>\n      <th>ZZZ3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249540</th>\n      <td>-0.012612</td>\n      <td>-0.033102</td>\n      <td>0.064470</td>\n      <td>-0.035743</td>\n      <td>0.046418</td>\n      <td>0.332813</td>\n      <td>0.187615</td>\n      <td>0.202148</td>\n      <td>0.148351</td>\n      <td>0.048806</td>\n      <td>...</td>\n      <td>0.023543</td>\n      <td>0.619112</td>\n      <td>0.457458</td>\n      <td>0.812111</td>\n      <td>0.452773</td>\n      <td>0.315088</td>\n      <td>0.449364</td>\n      <td>0.778671</td>\n      <td>0.264786</td>\n      <td>0.415394</td>\n    </tr>\n    <tr>\n      <th>441885</th>\n      <td>0.152488</td>\n      <td>1.444133</td>\n      <td>0.207721</td>\n      <td>0.522381</td>\n      <td>0.100024</td>\n      <td>0.235817</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.858773</td>\n      <td>0.569379</td>\n      <td>0.775311</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.686643</td>\n      <td>0.479418</td>\n      <td>0.435768</td>\n      <td>0.546380</td>\n    </tr>\n    <tr>\n      <th>441672</th>\n      <td>0.117885</td>\n      <td>0.322986</td>\n      <td>0.203585</td>\n      <td>0.253117</td>\n      <td>0.105531</td>\n      <td>0.306808</td>\n      <td>0.140186</td>\n      <td>0.170860</td>\n      <td>0.156711</td>\n      <td>0.256002</td>\n      <td>...</td>\n      <td>0.297475</td>\n      <td>0.561261</td>\n      <td>0.713150</td>\n      <td>0.557539</td>\n      <td>0.285374</td>\n      <td>0.245156</td>\n      <td>0.445551</td>\n      <td>0.624766</td>\n      <td>0.699554</td>\n      <td>0.363864</td>\n    </tr>\n    <tr>\n      <th>441746</th>\n      <td>0.126683</td>\n      <td>0.316034</td>\n      <td>0.207721</td>\n      <td>0.215761</td>\n      <td>0.100024</td>\n      <td>0.235817</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.668119</td>\n      <td>0.753780</td>\n      <td>0.646312</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.596188</td>\n      <td>0.364117</td>\n      <td>0.359216</td>\n      <td>0.623518</td>\n    </tr>\n    <tr>\n      <th>305151</th>\n      <td>0.123684</td>\n      <td>0.323163</td>\n      <td>0.198217</td>\n      <td>0.231361</td>\n      <td>0.100999</td>\n      <td>0.302301</td>\n      <td>0.135328</td>\n      <td>0.173449</td>\n      <td>0.155482</td>\n      <td>0.262875</td>\n      <td>...</td>\n      <td>0.279333</td>\n      <td>0.553355</td>\n      <td>0.509516</td>\n      <td>0.535441</td>\n      <td>0.244044</td>\n      <td>0.232352</td>\n      <td>0.489395</td>\n      <td>0.479349</td>\n      <td>0.679984</td>\n      <td>0.381019</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>441908</th>\n      <td>0.126683</td>\n      <td>0.316034</td>\n      <td>0.207721</td>\n      <td>0.241461</td>\n      <td>0.100024</td>\n      <td>0.230097</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.490617</td>\n      <td>0.353409</td>\n      <td>0.665294</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.407119</td>\n      <td>0.540433</td>\n      <td>0.512047</td>\n      <td>0.593405</td>\n    </tr>\n    <tr>\n      <th>441675</th>\n      <td>0.644383</td>\n      <td>0.322986</td>\n      <td>0.203585</td>\n      <td>0.253117</td>\n      <td>0.105531</td>\n      <td>0.159497</td>\n      <td>0.140186</td>\n      <td>0.170860</td>\n      <td>0.156711</td>\n      <td>0.490022</td>\n      <td>...</td>\n      <td>0.297475</td>\n      <td>0.441419</td>\n      <td>0.368896</td>\n      <td>0.391445</td>\n      <td>0.288958</td>\n      <td>0.245156</td>\n      <td>0.454411</td>\n      <td>0.403208</td>\n      <td>0.701180</td>\n      <td>0.330060</td>\n    </tr>\n    <tr>\n      <th>441765</th>\n      <td>0.126683</td>\n      <td>0.316034</td>\n      <td>0.207721</td>\n      <td>0.215768</td>\n      <td>0.100024</td>\n      <td>0.235817</td>\n      <td>0.135829</td>\n      <td>0.180823</td>\n      <td>0.157440</td>\n      <td>0.262362</td>\n      <td>...</td>\n      <td>0.286841</td>\n      <td>0.703829</td>\n      <td>0.633731</td>\n      <td>0.589447</td>\n      <td>0.244023</td>\n      <td>0.258659</td>\n      <td>0.469656</td>\n      <td>0.496578</td>\n      <td>0.570634</td>\n      <td>0.619272</td>\n    </tr>\n    <tr>\n      <th>305133</th>\n      <td>0.123684</td>\n      <td>0.323163</td>\n      <td>0.260336</td>\n      <td>0.231361</td>\n      <td>0.100999</td>\n      <td>0.247462</td>\n      <td>0.135328</td>\n      <td>0.173449</td>\n      <td>0.155482</td>\n      <td>0.262875</td>\n      <td>...</td>\n      <td>0.279333</td>\n      <td>0.394676</td>\n      <td>0.528906</td>\n      <td>0.507336</td>\n      <td>0.285471</td>\n      <td>0.298997</td>\n      <td>0.662130</td>\n      <td>0.609503</td>\n      <td>0.501518</td>\n      <td>0.316524</td>\n    </tr>\n    <tr>\n      <th>305238</th>\n      <td>0.123684</td>\n      <td>0.323163</td>\n      <td>0.198217</td>\n      <td>0.257054</td>\n      <td>0.109589</td>\n      <td>0.247462</td>\n      <td>0.135328</td>\n      <td>0.173449</td>\n      <td>0.155482</td>\n      <td>0.262875</td>\n      <td>...</td>\n      <td>0.279333</td>\n      <td>0.562625</td>\n      <td>0.478800</td>\n      <td>0.596472</td>\n      <td>0.224087</td>\n      <td>0.232352</td>\n      <td>0.483663</td>\n      <td>0.499843</td>\n      <td>0.337266</td>\n      <td>0.562459</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows × 8832 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "X_train_mm_q = min_max.fit_transform(X_train)\n",
    "X_train_mm_q = pd.DataFrame(X_train_mm_q, columns=X_train.columns, index=X_train.index)\n",
    "X_test_mm_q = min_max.transform(X_test)\n",
    "X_test_mm_q = pd.DataFrame(X_test_mm_q, columns=X_test.columns, index=X_test.index)\n",
    "X_test_mm_q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from pymrmre import mrmr\n",
    "\n",
    "# solutions_uni_q = mrmr.mrmr_ensemble(features=X_train_uni_q, targets=y_train.to_frame(), solution_length=100, solution_count=1)\n",
    "# solutions_ss_q = mrmr.mrmr_ensemble(features=X_train_ss_q, targets=y_train.to_frame(), solution_length=100, solution_count=1)\n",
    "# solutions_mm_q = mrmr.mrmr_ensemble(features=X_train_mm_q, targets=y_train.to_frame(), solution_length=100, solution_count=1)\n",
    "# solutions_g_norm = mrmr.mrmr_ensemble(features=X_train_g_norm_q, targets=y_train.to_frame(), solution_length=100, solution_count=1)\n",
    "solutions_g_ss_norm = mrmr.mrmr_ensemble(features=X_train_g_norm_ssq, targets=y_train.to_frame(), solution_length=100, solution_count=1)\n",
    "# feats_100_uni = solutions_uni_q[0][0]\n",
    "# feats_100_ss = solutions_ss_q[0][0]\n",
    "# feats_100_mm = solutions_mm_q[0][0]\n",
    "# feats_100_g_norm = solutions_g_norm[0][0]\n",
    "feats_100_g_ss = solutions_g_ss_norm[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "['CADPS_underexpr',\n 'SLC44A1_underexpr',\n 'CX3CR1_underexpr',\n 'EDC3_overexpr',\n 'ZNF192_overexpr',\n 'MB_overexpr',\n 'TSTA3_underexpr',\n 'GRB7_overexpr',\n 'ITIH1_underexpr',\n 'ATP6V1G1_overexpr']"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_100_g_ss[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:08:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:08:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 5.77 seconds.\n",
      "Best Score: 75.580%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.3, 'n_estimators': 120, 'min_child_weight': 2, 'max_depth': 4, 'max_delta_step': 4, 'learning_rate': 0.02, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:09:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.755804\n",
      "recall_0               0.666667\n",
      "precision_0            0.609332\n",
      "recall_1               0.844942\n",
      "precision_1            0.875033\n",
      "auc                    0.846888\n",
      "specificity            0.666667\n",
      "average_precision_0    0.222296\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.640983\n",
      "recall_0               0.450980\n",
      "precision_0            0.489362\n",
      "recall_1               0.830986\n",
      "precision_1            0.808219\n",
      "auc                    0.727700\n",
      "specificity            0.450980\n",
      "average_precision_0    0.224462\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    5.6s finished\n"
     ]
    }
   ],
   "source": [
    "params_uni_q_acc, clf_uni_q_acc, cv_score_uni_q_acc, test_scores_uni_q_acc = evaluate_ge((X_train_uni_q, X_test_uni_q, y_train, y_test), split=False, feats=feats_100_uni, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:09:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:09:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 2.79 seconds.\n",
      "Best Score: 74.529%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.3, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 5, 'max_delta_step': 4, 'learning_rate': 0.07, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:09:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.745291\n",
      "recall_0               0.633333\n",
      "precision_0            0.621396\n",
      "recall_1               0.857249\n",
      "precision_1            0.866197\n",
      "auc                    0.845643\n",
      "specificity            0.633333\n",
      "average_precision_0    0.220022\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.643745\n",
      "recall_0               0.470588\n",
      "precision_0            0.480000\n",
      "recall_1               0.816901\n",
      "precision_1            0.811189\n",
      "auc                    0.727285\n",
      "specificity            0.470588\n",
      "average_precision_0    0.224311\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "params_ss_q_acc, clf_ss_q_acc, cv_score_ss_q_acc, test_scores_ss_q_acc = evaluate_ge((X_train_ss_q, X_test_ss_q, y_train, y_test), split=False, feats=feats_100_ss, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:10:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:10:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 2.75 seconds.\n",
      "Best Score: 74.529%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.3, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 5, 'max_delta_step': 4, 'learning_rate': 0.07, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:10:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.745291\n",
      "recall_0               0.633333\n",
      "precision_0            0.621396\n",
      "recall_1               0.857249\n",
      "precision_1            0.866197\n",
      "auc                    0.845643\n",
      "specificity            0.633333\n",
      "average_precision_0    0.220022\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.643745\n",
      "recall_0               0.470588\n",
      "precision_0            0.480000\n",
      "recall_1               0.816901\n",
      "precision_1            0.811189\n",
      "auc                    0.727285\n",
      "specificity            0.470588\n",
      "average_precision_0    0.224311\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of 125 | elapsed:    2.0s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    2.6s finished\n"
     ]
    }
   ],
   "source": [
    "params_mm_q_acc, clf_mm_q_acc, cv_score_mm_q_acc, test_scores_mm_q_acc = evaluate_ge((X_train_mm_q, X_test_mm_q, y_train, y_test), split=False, feats=feats_100_mm, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:21:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 3.38 seconds.\n",
      "Best Score: 76.115%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.3, 'n_estimators': 120, 'min_child_weight': 2, 'max_depth': 4, 'max_delta_step': 4, 'learning_rate': 0.02, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:21:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.761154\n",
      "recall_0               0.683333\n",
      "precision_0            0.606025\n",
      "recall_1               0.838974\n",
      "precision_1            0.880254\n",
      "auc                    0.846286\n",
      "specificity            0.683333\n",
      "average_precision_0    0.223649\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.642985\n",
      "recall_0               0.490196\n",
      "precision_0            0.462963\n",
      "recall_1               0.795775\n",
      "precision_1            0.812950\n",
      "auc                    0.715686\n",
      "specificity            0.490196\n",
      "average_precision_0    0.224893\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    3.2s finished\n"
     ]
    }
   ],
   "source": [
    "params_g_norm_acc, clf_g_norm_acc, cv_score_g_norm_acc, test_scores_g_norm_acc = evaluate_ge((X_train_g_norm_q, X_test_g_norm_q, y_train, y_test), split=False, feats=feats_100_g_norm, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Acc\n",
      "\n",
      "\t\t\tUniform\t\t\t\tNormal\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t75.58%\t\t\t\t76.12%\n",
      "\n",
      "recall_0:\t\t66.67%\t\t\t\t68.33%\n",
      "\n",
      "precision_0:\t\t60.93%\t\t\t\t60.60%\n",
      "\n",
      "recall_1:\t\t84.49%\t\t\t\t83.90%\n",
      "\n",
      "precision_1:\t\t87.50%\t\t\t\t88.03%\n",
      "\n",
      "auc:\t\t\t84.69%\t\t\t\t84.63%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Acc\n",
      "\n",
      "\t\t\tUniform\t\t\t\tNormal\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t64.10%\t\t\t\t64.30%\n",
      "\n",
      "recall_0:\t\t45.10%\t\t\t\t49.02%\n",
      "\n",
      "precision_0:\t\t48.94%\t\t\t\t46.30%\n",
      "\n",
      "recall_1:\t\t83.10%\t\t\t\t79.58%\n",
      "\n",
      "precision_1:\t\t80.82%\t\t\t\t81.29%\n",
      "\n",
      "auc:\t\t\t72.77%\t\t\t\t71.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score_comparison_v2(cv_score_uni_q_acc, cv_score_g_norm_acc, test_scores_uni_q_acc, test_scores_g_norm_acc, header_1=\"Uniform\", header_2=\"Normal\", opt=\"Acc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Acc\n",
      "\n",
      "\t\t\tUniform\t\t\t\tSTD Scaler\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t75.58%\t\t\t\t74.53%\n",
      "\n",
      "recall_0:\t\t66.67%\t\t\t\t63.33%\n",
      "\n",
      "precision_0:\t\t60.93%\t\t\t\t62.14%\n",
      "\n",
      "recall_1:\t\t84.49%\t\t\t\t85.72%\n",
      "\n",
      "precision_1:\t\t87.50%\t\t\t\t86.62%\n",
      "\n",
      "auc:\t\t\t84.69%\t\t\t\t84.56%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Acc\n",
      "\n",
      "\t\t\tUniform\t\t\t\tSTD Scaler\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t64.10%\t\t\t\t64.37%\n",
      "\n",
      "recall_0:\t\t45.10%\t\t\t\t47.06%\n",
      "\n",
      "precision_0:\t\t48.94%\t\t\t\t48.00%\n",
      "\n",
      "recall_1:\t\t83.10%\t\t\t\t81.69%\n",
      "\n",
      "precision_1:\t\t80.82%\t\t\t\t81.12%\n",
      "\n",
      "auc:\t\t\t72.77%\t\t\t\t72.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score_comparison_v2(cv_score_uni_q_acc, cv_score_ss_q_acc, test_scores_uni_q_acc, test_scores_ss_q_acc, header_1=\"Uniform\", header_2=\"STD Scaler\", opt=\"Acc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Acc\n",
      "\n",
      "\t\t\tUniform\t\t\t\tMinMax Scaler\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t75.58%\t\t\t\t74.53%\n",
      "\n",
      "recall_0:\t\t66.67%\t\t\t\t63.33%\n",
      "\n",
      "precision_0:\t\t60.93%\t\t\t\t62.14%\n",
      "\n",
      "recall_1:\t\t84.49%\t\t\t\t85.72%\n",
      "\n",
      "precision_1:\t\t87.50%\t\t\t\t86.62%\n",
      "\n",
      "auc:\t\t\t84.69%\t\t\t\t84.56%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Acc\n",
      "\n",
      "\t\t\tUniform\t\t\t\tMinMax Scaler\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t64.10%\t\t\t\t64.37%\n",
      "\n",
      "recall_0:\t\t45.10%\t\t\t\t47.06%\n",
      "\n",
      "precision_0:\t\t48.94%\t\t\t\t48.00%\n",
      "\n",
      "recall_1:\t\t83.10%\t\t\t\t81.69%\n",
      "\n",
      "precision_1:\t\t80.82%\t\t\t\t81.12%\n",
      "\n",
      "auc:\t\t\t72.77%\t\t\t\t72.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score_comparison_v2(cv_score_uni_q_acc, cv_score_mm_q_acc, test_scores_uni_q_acc, test_scores_mm_q_acc, header_1=\"Uniform\", header_2=\"MinMax Scaler\", opt=\"Acc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}