{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xabush/pln-brca-xp/notebooks\n"
     ]
    }
   ],
   "source": [
    "#In this notebook:\n",
    "#1. This notebook compares Raw Model with Embedding  of train on absolute gene expression\n",
    "#2. Creates a stacked ensemble of Raw and Embedding models\n",
    "%cd \"~/pln-brca-xp/notebooks\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         MAGEA12   MAGEA11      KLF1      ADH7      MSH4     BIRC3    AKR1C4  \\\n22473   3.402084  3.494320  4.023385  3.541183  3.832442  6.952044  3.855832   \n249608  4.187661  3.866810  4.085393  3.139782  4.328903  5.025544  3.661857   \n249599  4.837456  4.006630  3.761547  3.688134  4.285157  6.048967  3.363964   \n441791  3.187184  3.628224  3.539075  3.587210  3.619540  5.592908  3.491800   \n249560  3.374551  3.497133  3.661740  3.460654  3.436120  5.450084  3.396684   \n...          ...       ...       ...       ...       ...       ...       ...   \n22502   3.791553  3.662776  4.079825  3.758397  3.395359  5.764780  3.540518   \n441685  3.085443  3.653459  4.123113  3.680947  3.647998  5.939426  3.512556   \n441899  3.187184  3.628224  3.539075  4.944182  3.619540  5.592908  3.491800   \n305219  3.152498  3.654102  3.478021  3.613843  3.624577  7.054350  3.489415   \n249618  3.056220  3.809811  3.532921  3.920689  3.614089  4.827825  3.459768   \n\n            GBX2      GCGR   SIGLEC9  ...     ZNF80     ZNF83     ZNF84  \\\n22473   4.082446  3.627827  4.335390  ...  3.974718  5.558950  5.514646   \n249608  4.191745  3.685930  4.569808  ...  3.842874  6.804398  7.363036   \n249599  3.470209  3.483006  4.521905  ...  3.334548  7.619913  7.060920   \n441791  3.593392  3.615120  4.072719  ...  3.500689  7.447879  5.975008   \n249560  3.840728  3.782517  4.735773  ...  3.867339  5.562545  6.554106   \n...          ...       ...       ...  ...       ...       ...       ...   \n22502   3.281390  3.479762  4.149927  ...  3.464084  6.851393  6.865060   \n441685  3.546215  3.612820  4.027431  ...  3.549222  5.925519  5.060669   \n441899  3.593392  3.615120  4.072719  ...  3.500689  7.269916  7.222344   \n305219  3.558475  3.608941  4.076373  ...  3.466420  6.817555  6.496405   \n249618  3.495976  3.923674  4.117954  ...  3.289117  8.117914  6.950398   \n\n            ZNF91    ZNHIT2    ZSCAN2      ZXDC       ZYX     ZZEF1      ZZZ3  \n22473    6.841577  3.183982  4.412229  6.200891  6.647563  6.251839  5.097675  \n249608   8.035668  4.077028  3.948575  6.492446  5.693120  6.294657  5.586593  \n249599   8.144723  3.051310  3.543237  5.964417  6.366522  6.379608  6.199579  \n441791   8.160910  3.569871  3.626728  6.146075  6.470966  5.285979  6.637736  \n249560   7.087424  3.667209  3.522804  7.350276  6.389579  6.662883  4.938632  \n...           ...       ...       ...       ...       ...       ...       ...  \n22502    9.865863  3.543129  3.991906  6.949398  7.204528  5.047344  6.525453  \n441685   6.356471  3.653427  3.766725  6.038282  5.789259  7.032159  6.530652  \n441899  10.263361  3.569871  3.626728  6.107571  7.614689  6.006757  5.465672  \n305219   9.184417  3.420080  3.521218  5.430783  7.427696  5.050724  6.400798  \n249618   7.938085  3.184268  3.470015  5.723205  6.588195  5.548664  6.518136  \n\n[449 rows x 8832 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAGEA12</th>\n      <th>MAGEA11</th>\n      <th>KLF1</th>\n      <th>ADH7</th>\n      <th>MSH4</th>\n      <th>BIRC3</th>\n      <th>AKR1C4</th>\n      <th>GBX2</th>\n      <th>GCGR</th>\n      <th>SIGLEC9</th>\n      <th>...</th>\n      <th>ZNF80</th>\n      <th>ZNF83</th>\n      <th>ZNF84</th>\n      <th>ZNF91</th>\n      <th>ZNHIT2</th>\n      <th>ZSCAN2</th>\n      <th>ZXDC</th>\n      <th>ZYX</th>\n      <th>ZZEF1</th>\n      <th>ZZZ3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22473</th>\n      <td>3.402084</td>\n      <td>3.494320</td>\n      <td>4.023385</td>\n      <td>3.541183</td>\n      <td>3.832442</td>\n      <td>6.952044</td>\n      <td>3.855832</td>\n      <td>4.082446</td>\n      <td>3.627827</td>\n      <td>4.335390</td>\n      <td>...</td>\n      <td>3.974718</td>\n      <td>5.558950</td>\n      <td>5.514646</td>\n      <td>6.841577</td>\n      <td>3.183982</td>\n      <td>4.412229</td>\n      <td>6.200891</td>\n      <td>6.647563</td>\n      <td>6.251839</td>\n      <td>5.097675</td>\n    </tr>\n    <tr>\n      <th>249608</th>\n      <td>4.187661</td>\n      <td>3.866810</td>\n      <td>4.085393</td>\n      <td>3.139782</td>\n      <td>4.328903</td>\n      <td>5.025544</td>\n      <td>3.661857</td>\n      <td>4.191745</td>\n      <td>3.685930</td>\n      <td>4.569808</td>\n      <td>...</td>\n      <td>3.842874</td>\n      <td>6.804398</td>\n      <td>7.363036</td>\n      <td>8.035668</td>\n      <td>4.077028</td>\n      <td>3.948575</td>\n      <td>6.492446</td>\n      <td>5.693120</td>\n      <td>6.294657</td>\n      <td>5.586593</td>\n    </tr>\n    <tr>\n      <th>249599</th>\n      <td>4.837456</td>\n      <td>4.006630</td>\n      <td>3.761547</td>\n      <td>3.688134</td>\n      <td>4.285157</td>\n      <td>6.048967</td>\n      <td>3.363964</td>\n      <td>3.470209</td>\n      <td>3.483006</td>\n      <td>4.521905</td>\n      <td>...</td>\n      <td>3.334548</td>\n      <td>7.619913</td>\n      <td>7.060920</td>\n      <td>8.144723</td>\n      <td>3.051310</td>\n      <td>3.543237</td>\n      <td>5.964417</td>\n      <td>6.366522</td>\n      <td>6.379608</td>\n      <td>6.199579</td>\n    </tr>\n    <tr>\n      <th>441791</th>\n      <td>3.187184</td>\n      <td>3.628224</td>\n      <td>3.539075</td>\n      <td>3.587210</td>\n      <td>3.619540</td>\n      <td>5.592908</td>\n      <td>3.491800</td>\n      <td>3.593392</td>\n      <td>3.615120</td>\n      <td>4.072719</td>\n      <td>...</td>\n      <td>3.500689</td>\n      <td>7.447879</td>\n      <td>5.975008</td>\n      <td>8.160910</td>\n      <td>3.569871</td>\n      <td>3.626728</td>\n      <td>6.146075</td>\n      <td>6.470966</td>\n      <td>5.285979</td>\n      <td>6.637736</td>\n    </tr>\n    <tr>\n      <th>249560</th>\n      <td>3.374551</td>\n      <td>3.497133</td>\n      <td>3.661740</td>\n      <td>3.460654</td>\n      <td>3.436120</td>\n      <td>5.450084</td>\n      <td>3.396684</td>\n      <td>3.840728</td>\n      <td>3.782517</td>\n      <td>4.735773</td>\n      <td>...</td>\n      <td>3.867339</td>\n      <td>5.562545</td>\n      <td>6.554106</td>\n      <td>7.087424</td>\n      <td>3.667209</td>\n      <td>3.522804</td>\n      <td>7.350276</td>\n      <td>6.389579</td>\n      <td>6.662883</td>\n      <td>4.938632</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22502</th>\n      <td>3.791553</td>\n      <td>3.662776</td>\n      <td>4.079825</td>\n      <td>3.758397</td>\n      <td>3.395359</td>\n      <td>5.764780</td>\n      <td>3.540518</td>\n      <td>3.281390</td>\n      <td>3.479762</td>\n      <td>4.149927</td>\n      <td>...</td>\n      <td>3.464084</td>\n      <td>6.851393</td>\n      <td>6.865060</td>\n      <td>9.865863</td>\n      <td>3.543129</td>\n      <td>3.991906</td>\n      <td>6.949398</td>\n      <td>7.204528</td>\n      <td>5.047344</td>\n      <td>6.525453</td>\n    </tr>\n    <tr>\n      <th>441685</th>\n      <td>3.085443</td>\n      <td>3.653459</td>\n      <td>4.123113</td>\n      <td>3.680947</td>\n      <td>3.647998</td>\n      <td>5.939426</td>\n      <td>3.512556</td>\n      <td>3.546215</td>\n      <td>3.612820</td>\n      <td>4.027431</td>\n      <td>...</td>\n      <td>3.549222</td>\n      <td>5.925519</td>\n      <td>5.060669</td>\n      <td>6.356471</td>\n      <td>3.653427</td>\n      <td>3.766725</td>\n      <td>6.038282</td>\n      <td>5.789259</td>\n      <td>7.032159</td>\n      <td>6.530652</td>\n    </tr>\n    <tr>\n      <th>441899</th>\n      <td>3.187184</td>\n      <td>3.628224</td>\n      <td>3.539075</td>\n      <td>4.944182</td>\n      <td>3.619540</td>\n      <td>5.592908</td>\n      <td>3.491800</td>\n      <td>3.593392</td>\n      <td>3.615120</td>\n      <td>4.072719</td>\n      <td>...</td>\n      <td>3.500689</td>\n      <td>7.269916</td>\n      <td>7.222344</td>\n      <td>10.263361</td>\n      <td>3.569871</td>\n      <td>3.626728</td>\n      <td>6.107571</td>\n      <td>7.614689</td>\n      <td>6.006757</td>\n      <td>5.465672</td>\n    </tr>\n    <tr>\n      <th>305219</th>\n      <td>3.152498</td>\n      <td>3.654102</td>\n      <td>3.478021</td>\n      <td>3.613843</td>\n      <td>3.624577</td>\n      <td>7.054350</td>\n      <td>3.489415</td>\n      <td>3.558475</td>\n      <td>3.608941</td>\n      <td>4.076373</td>\n      <td>...</td>\n      <td>3.466420</td>\n      <td>6.817555</td>\n      <td>6.496405</td>\n      <td>9.184417</td>\n      <td>3.420080</td>\n      <td>3.521218</td>\n      <td>5.430783</td>\n      <td>7.427696</td>\n      <td>5.050724</td>\n      <td>6.400798</td>\n    </tr>\n    <tr>\n      <th>249618</th>\n      <td>3.056220</td>\n      <td>3.809811</td>\n      <td>3.532921</td>\n      <td>3.920689</td>\n      <td>3.614089</td>\n      <td>4.827825</td>\n      <td>3.459768</td>\n      <td>3.495976</td>\n      <td>3.923674</td>\n      <td>4.117954</td>\n      <td>...</td>\n      <td>3.289117</td>\n      <td>8.117914</td>\n      <td>6.950398</td>\n      <td>7.938085</td>\n      <td>3.184268</td>\n      <td>3.470015</td>\n      <td>5.723205</td>\n      <td>6.588195</td>\n      <td>5.548664</td>\n      <td>6.518136</td>\n    </tr>\n  </tbody>\n</table>\n<p>449 rows × 8832 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_df = pd.read_csv(\"/var/www/datasets/merged-combat15.csv.xz\", index_col=\"patient_ID\")\n",
    "state_df = pd.read_csv(\"/var/www/datasets/embedding_vector_state_and_outcome.csv\", index_col=\"patient_ID\")\n",
    "ge_out_df = ge_df.join(state_df[\"posOutcome\"])\n",
    "X_ge_train, X_ge_test, y_ge_train, y_ge_test = get_train_test_set(ge_out_df, \"/var/www/datasets/tamox_train_set.txt\", \"/var/www/datasets/tamox_test_set.txt\")\n",
    "X_ge_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "               0         1         2         3         4         5         6  \\\n22473   0.237823 -0.059521  0.230957  0.050294  0.133526  0.028123 -0.002614   \n249608 -0.173263 -0.187113 -0.001124  0.037108 -0.008250 -0.087975 -0.006915   \n249599  0.201768 -0.029456  0.182885 -0.049242 -0.279091  0.298254  0.033760   \n441791 -0.140808  0.109703  0.027210 -0.068730 -0.141512  0.023152  0.034641   \n249560  0.069039  0.031378  0.168199  0.071899 -0.042612  0.176539 -0.052509   \n...          ...       ...       ...       ...       ...       ...       ...   \n22502   0.024670 -0.161788 -0.091536 -0.177791  0.074665  0.146178  0.034686   \n441685  0.162798 -0.146114  0.247207 -0.086093  0.131794 -0.152520  0.106691   \n441899  0.083137  0.023575  0.193433 -0.057129 -0.029097 -0.055911 -0.057442   \n305219 -0.144892  0.275208 -0.052561  0.044638 -0.142087  0.049335  0.185052   \n249618 -0.081991 -0.143039  0.142324 -0.113061  0.021914  0.141745 -0.140986   \n\n               7         8         9  ...       438       439       440  \\\n22473   0.066303 -0.125752 -0.008396  ... -0.000359 -0.002639  0.015120   \n249608 -0.115483  0.048986  0.285757  ...  0.009936  0.002214 -0.003510   \n249599  0.065236 -0.084633  0.000219  ... -0.002821 -0.000035  0.004231   \n441791 -0.068911 -0.065362 -0.038813  ...  0.002569 -0.003972 -0.001924   \n249560 -0.108884 -0.069096  0.088603  ...  0.003032 -0.011236 -0.005964   \n...          ...       ...       ...  ...       ...       ...       ...   \n22502   0.010591 -0.096995 -0.057871  ...  0.006561 -0.006026  0.000400   \n441685  0.095469  0.072779  0.005359  ... -0.012960 -0.002213 -0.002588   \n441899 -0.070603 -0.167829 -0.124643  ... -0.000217 -0.005835  0.001702   \n305219 -0.116351 -0.229087  0.056807  ...  0.002822 -0.001944  0.002204   \n249618  0.068668 -0.093218 -0.254645  ... -0.003217  0.003832 -0.000078   \n\n             441       442       443       444       445       446  \\\n22473   0.003535  0.010068  0.003408 -0.001064 -0.000563 -0.003197   \n249608  0.002198  0.001158 -0.000322 -0.002706  0.003647 -0.000714   \n249599 -0.002739 -0.008983 -0.006409  0.002831 -0.001213 -0.000136   \n441791  0.002100 -0.001682 -0.001704  0.000939 -0.000230  0.000522   \n249560  0.002140  0.006396  0.013673  0.004449  0.005885 -0.002776   \n...          ...       ...       ...       ...       ...       ...   \n22502   0.004194  0.001375  0.004093  0.001981 -0.003848  0.002427   \n441685  0.013510 -0.006051  0.004944  0.005721  0.002523 -0.000135   \n441899  0.002454 -0.005235 -0.016558 -0.000003 -0.000213 -0.001711   \n305219  0.002188 -0.004815 -0.003226  0.001900 -0.000100 -0.000391   \n249618 -0.000067 -0.002633 -0.008476  0.000069  0.000097  0.001509   \n\n                 447  \n22473  -2.881978e-06  \n249608  1.604505e-05  \n249599  1.924525e-05  \n441791 -5.176709e-06  \n249560  1.919820e-06  \n...              ...  \n22502  -1.241575e-05  \n441685  1.531776e-05  \n441899  4.043089e-05  \n305219 -2.115106e-07  \n249618 -7.470116e-06  \n\n[449 rows x 448 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>438</th>\n      <th>439</th>\n      <th>440</th>\n      <th>441</th>\n      <th>442</th>\n      <th>443</th>\n      <th>444</th>\n      <th>445</th>\n      <th>446</th>\n      <th>447</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22473</th>\n      <td>0.237823</td>\n      <td>-0.059521</td>\n      <td>0.230957</td>\n      <td>0.050294</td>\n      <td>0.133526</td>\n      <td>0.028123</td>\n      <td>-0.002614</td>\n      <td>0.066303</td>\n      <td>-0.125752</td>\n      <td>-0.008396</td>\n      <td>...</td>\n      <td>-0.000359</td>\n      <td>-0.002639</td>\n      <td>0.015120</td>\n      <td>0.003535</td>\n      <td>0.010068</td>\n      <td>0.003408</td>\n      <td>-0.001064</td>\n      <td>-0.000563</td>\n      <td>-0.003197</td>\n      <td>-2.881978e-06</td>\n    </tr>\n    <tr>\n      <th>249608</th>\n      <td>-0.173263</td>\n      <td>-0.187113</td>\n      <td>-0.001124</td>\n      <td>0.037108</td>\n      <td>-0.008250</td>\n      <td>-0.087975</td>\n      <td>-0.006915</td>\n      <td>-0.115483</td>\n      <td>0.048986</td>\n      <td>0.285757</td>\n      <td>...</td>\n      <td>0.009936</td>\n      <td>0.002214</td>\n      <td>-0.003510</td>\n      <td>0.002198</td>\n      <td>0.001158</td>\n      <td>-0.000322</td>\n      <td>-0.002706</td>\n      <td>0.003647</td>\n      <td>-0.000714</td>\n      <td>1.604505e-05</td>\n    </tr>\n    <tr>\n      <th>249599</th>\n      <td>0.201768</td>\n      <td>-0.029456</td>\n      <td>0.182885</td>\n      <td>-0.049242</td>\n      <td>-0.279091</td>\n      <td>0.298254</td>\n      <td>0.033760</td>\n      <td>0.065236</td>\n      <td>-0.084633</td>\n      <td>0.000219</td>\n      <td>...</td>\n      <td>-0.002821</td>\n      <td>-0.000035</td>\n      <td>0.004231</td>\n      <td>-0.002739</td>\n      <td>-0.008983</td>\n      <td>-0.006409</td>\n      <td>0.002831</td>\n      <td>-0.001213</td>\n      <td>-0.000136</td>\n      <td>1.924525e-05</td>\n    </tr>\n    <tr>\n      <th>441791</th>\n      <td>-0.140808</td>\n      <td>0.109703</td>\n      <td>0.027210</td>\n      <td>-0.068730</td>\n      <td>-0.141512</td>\n      <td>0.023152</td>\n      <td>0.034641</td>\n      <td>-0.068911</td>\n      <td>-0.065362</td>\n      <td>-0.038813</td>\n      <td>...</td>\n      <td>0.002569</td>\n      <td>-0.003972</td>\n      <td>-0.001924</td>\n      <td>0.002100</td>\n      <td>-0.001682</td>\n      <td>-0.001704</td>\n      <td>0.000939</td>\n      <td>-0.000230</td>\n      <td>0.000522</td>\n      <td>-5.176709e-06</td>\n    </tr>\n    <tr>\n      <th>249560</th>\n      <td>0.069039</td>\n      <td>0.031378</td>\n      <td>0.168199</td>\n      <td>0.071899</td>\n      <td>-0.042612</td>\n      <td>0.176539</td>\n      <td>-0.052509</td>\n      <td>-0.108884</td>\n      <td>-0.069096</td>\n      <td>0.088603</td>\n      <td>...</td>\n      <td>0.003032</td>\n      <td>-0.011236</td>\n      <td>-0.005964</td>\n      <td>0.002140</td>\n      <td>0.006396</td>\n      <td>0.013673</td>\n      <td>0.004449</td>\n      <td>0.005885</td>\n      <td>-0.002776</td>\n      <td>1.919820e-06</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22502</th>\n      <td>0.024670</td>\n      <td>-0.161788</td>\n      <td>-0.091536</td>\n      <td>-0.177791</td>\n      <td>0.074665</td>\n      <td>0.146178</td>\n      <td>0.034686</td>\n      <td>0.010591</td>\n      <td>-0.096995</td>\n      <td>-0.057871</td>\n      <td>...</td>\n      <td>0.006561</td>\n      <td>-0.006026</td>\n      <td>0.000400</td>\n      <td>0.004194</td>\n      <td>0.001375</td>\n      <td>0.004093</td>\n      <td>0.001981</td>\n      <td>-0.003848</td>\n      <td>0.002427</td>\n      <td>-1.241575e-05</td>\n    </tr>\n    <tr>\n      <th>441685</th>\n      <td>0.162798</td>\n      <td>-0.146114</td>\n      <td>0.247207</td>\n      <td>-0.086093</td>\n      <td>0.131794</td>\n      <td>-0.152520</td>\n      <td>0.106691</td>\n      <td>0.095469</td>\n      <td>0.072779</td>\n      <td>0.005359</td>\n      <td>...</td>\n      <td>-0.012960</td>\n      <td>-0.002213</td>\n      <td>-0.002588</td>\n      <td>0.013510</td>\n      <td>-0.006051</td>\n      <td>0.004944</td>\n      <td>0.005721</td>\n      <td>0.002523</td>\n      <td>-0.000135</td>\n      <td>1.531776e-05</td>\n    </tr>\n    <tr>\n      <th>441899</th>\n      <td>0.083137</td>\n      <td>0.023575</td>\n      <td>0.193433</td>\n      <td>-0.057129</td>\n      <td>-0.029097</td>\n      <td>-0.055911</td>\n      <td>-0.057442</td>\n      <td>-0.070603</td>\n      <td>-0.167829</td>\n      <td>-0.124643</td>\n      <td>...</td>\n      <td>-0.000217</td>\n      <td>-0.005835</td>\n      <td>0.001702</td>\n      <td>0.002454</td>\n      <td>-0.005235</td>\n      <td>-0.016558</td>\n      <td>-0.000003</td>\n      <td>-0.000213</td>\n      <td>-0.001711</td>\n      <td>4.043089e-05</td>\n    </tr>\n    <tr>\n      <th>305219</th>\n      <td>-0.144892</td>\n      <td>0.275208</td>\n      <td>-0.052561</td>\n      <td>0.044638</td>\n      <td>-0.142087</td>\n      <td>0.049335</td>\n      <td>0.185052</td>\n      <td>-0.116351</td>\n      <td>-0.229087</td>\n      <td>0.056807</td>\n      <td>...</td>\n      <td>0.002822</td>\n      <td>-0.001944</td>\n      <td>0.002204</td>\n      <td>0.002188</td>\n      <td>-0.004815</td>\n      <td>-0.003226</td>\n      <td>0.001900</td>\n      <td>-0.000100</td>\n      <td>-0.000391</td>\n      <td>-2.115106e-07</td>\n    </tr>\n    <tr>\n      <th>249618</th>\n      <td>-0.081991</td>\n      <td>-0.143039</td>\n      <td>0.142324</td>\n      <td>-0.113061</td>\n      <td>0.021914</td>\n      <td>0.141745</td>\n      <td>-0.140986</td>\n      <td>0.068668</td>\n      <td>-0.093218</td>\n      <td>-0.254645</td>\n      <td>...</td>\n      <td>-0.003217</td>\n      <td>0.003832</td>\n      <td>-0.000078</td>\n      <td>-0.000067</td>\n      <td>-0.002633</td>\n      <td>-0.008476</td>\n      <td>0.000069</td>\n      <td>0.000097</td>\n      <td>0.001509</td>\n      <td>-7.470116e-06</td>\n    </tr>\n  </tbody>\n</table>\n<p>449 rows × 448 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb_abs_df = pd.read_csv(\"/var/www/datasets/embedding-vectors/tamoxifen_group/v2/absolute/property_vector_ge_TRAIN_2021-03-04.csv\", sep=\"\\t\", index_col=\"patient_ID\")\n",
    "train_emb_abs_outcome_df = train_emb_abs_df.join(ge_out_df[\"posOutcome\"])\n",
    "X_train_emb_abs_df, y_train_emb_abs_df = train_emb_abs_outcome_df.drop([\"posOutcome\"], axis=1), train_emb_abs_outcome_df[\"posOutcome\"]\n",
    "X_train_emb_abs_df, y_train_emb_abs_df = X_train_emb_abs_df.loc[y_ge_train.index,:], y_train_emb_abs_df.loc[y_ge_train.index]\n",
    "X_train_emb_abs_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "test_emb_abs_df = pd.read_csv(\"/var/www/datasets/embedding-vectors/tamoxifen_group/v2/absolute/property_vector_ge_TEST_2021-03-04.csv\", sep=\"\\t\", index_col=\"patient_ID\")\n",
    "test_emb_abs_outcome_df = test_emb_abs_df.join(ge_out_df[\"posOutcome\"])\n",
    "X_test_emb_abs_df, y_test_emb_abs_df = test_emb_abs_outcome_df.drop([\"posOutcome\"], axis=1), test_emb_abs_outcome_df[\"posOutcome\"]\n",
    "X_test_emb_abs_df, y_test_emb_abs_df = X_test_emb_abs_df.loc[y_ge_test.index,:], y_test_emb_abs_df[y_ge_test.index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_emb_abs_34_dim, X_test_emb_abs_34_dim = X_train_emb_abs_df.iloc[:, :34], X_test_emb_abs_df.iloc[:, :34]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "feats_100 = load_features(\"/var/www/datasets/feats_100_raw_nn.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[12:10:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:10:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 3.99 seconds.\n",
      "Best Score: 74.529%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.3, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 5, 'max_delta_step': 4, 'learning_rate': 0.07, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[12:10:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.745291\n",
      "recall_0               0.633333\n",
      "precision_0            0.621396\n",
      "recall_1               0.857249\n",
      "precision_1            0.866197\n",
      "auc                    0.845643\n",
      "specificity            0.633333\n",
      "average_precision_0    0.220022\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.643745\n",
      "recall_0               0.470588\n",
      "precision_0            0.480000\n",
      "recall_1               0.816901\n",
      "precision_1            0.811189\n",
      "auc                    0.727285\n",
      "specificity            0.470588\n",
      "average_precision_0    0.224311\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    3.8s finished\n"
     ]
    }
   ],
   "source": [
    "params_raw_acc, clf_raw_acc, cv_score_raw_acc, test_scores_raw_acc = evaluate_ge((X_ge_train, X_ge_test, y_ge_train, y_ge_test), split=False, feats=feats_100, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[15:42:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:42:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 26.75 seconds.\n",
      "Best Score: 76.906%\n",
      "{'subsample': 1.0, 'scale_pos_weight': 0.3, 'n_estimators': 20, 'min_child_weight': 4, 'max_depth': 6, 'max_delta_step': 4, 'learning_rate': 0.05, 'gamma': 1.5, 'colsample_bytree': 0.6}\n",
      "[15:42:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.769062\n",
      "recall_0               0.708333\n",
      "precision_0            0.603362\n",
      "recall_1               0.829790\n",
      "precision_1            0.886577\n",
      "auc                    0.812723\n",
      "specificity            0.708333\n",
      "average_precision_0    0.225584\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.652789\n",
      "recall_0               0.509804\n",
      "precision_0            0.472727\n",
      "recall_1               0.795775\n",
      "precision_1            0.818841\n",
      "auc                    0.691384\n",
      "specificity            0.509804\n",
      "average_precision_0    0.223519\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   26.2s finished\n"
     ]
    }
   ],
   "source": [
    "params_emb_acc, clf_emb_acc, cv_score_emb_acc, test_scores_emb_acc = evaluate_ge((X_train_emb_abs_df, X_test_emb_abs_df, y_ge_train, y_ge_test), split=False, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[12:12:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:12:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 1.62 seconds.\n",
      "Best Score: 78.057%\n",
      "{'subsample': 0.6, 'scale_pos_weight': 0.1, 'n_estimators': 120, 'min_child_weight': 1, 'max_depth': 5, 'max_delta_step': 3, 'learning_rate': 0.07, 'gamma': 1, 'colsample_bytree': 0.8}\n",
      "[12:12:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.780571\n",
      "recall_0               0.883333\n",
      "precision_0            0.502409\n",
      "recall_1               0.677809\n",
      "precision_1            0.943132\n",
      "auc                    0.857117\n",
      "specificity            0.883333\n",
      "average_precision_0    0.245460\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.664803\n",
      "recall_0               0.745098\n",
      "precision_0            0.391753\n",
      "recall_1               0.584507\n",
      "precision_1            0.864583\n",
      "auc                    0.710577\n",
      "specificity            0.745098\n",
      "average_precision_0    0.231409\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of 125 | elapsed:    1.1s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "params_emb_34_acc, clf_emb_34_acc, cv_score_emb_34_acc, test_scores_emb_34_acc = evaluate_ge((X_train_emb_abs_34_dim, X_test_emb_abs_34_dim, y_ge_train, y_ge_test), split=False, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Balanced Opt\n",
      "\n",
      "\t\t\tRaw\t\t\t\tEmb (34 dims)\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t74.53%\t\t\t\t78.06%\n",
      "\n",
      "recall_0:\t\t63.33%\t\t\t\t88.33%\n",
      "\n",
      "precision_0:\t\t62.14%\t\t\t\t50.24%\n",
      "\n",
      "recall_1:\t\t85.72%\t\t\t\t67.78%\n",
      "\n",
      "precision_1:\t\t86.62%\t\t\t\t94.31%\n",
      "\n",
      "auc:\t\t\t84.56%\t\t\t\t85.71%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Balanced Opt\n",
      "\n",
      "\t\t\tRaw\t\t\t\tEmb (34 dims)\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t64.37%\t\t\t\t66.48%\n",
      "\n",
      "recall_0:\t\t47.06%\t\t\t\t74.51%\n",
      "\n",
      "precision_0:\t\t48.00%\t\t\t\t39.18%\n",
      "\n",
      "recall_1:\t\t81.69%\t\t\t\t58.45%\n",
      "\n",
      "precision_1:\t\t81.12%\t\t\t\t86.46%\n",
      "\n",
      "auc:\t\t\t72.73%\t\t\t\t71.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare raw model with embedding model\n",
    "print_score_comparison_v2(cv_score_raw_acc, cv_score_emb_34_acc, test_scores_raw_acc, test_scores_emb_34_acc, header_1=\"Raw\", header_2=\"Emb (34 dims)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "           MYH7B    ADRA2A      GRB7     LRP12      MSRA    MRPL17      HSPB3  \\\n249540  4.081220  5.513807  4.859041  5.341824  4.396074  9.662919   2.676551   \n441885  3.346856  4.321042  4.915306  3.316025  5.162939  6.609321   2.824619   \n441672  3.344460  4.884633  9.315200  3.228507  4.330381  9.679237   2.803146   \n441746  3.346856  5.651840  4.915306  3.316025  4.950791  8.296965   2.824619   \n305151  3.346433  6.204796  4.894265  3.231599  5.866692  8.514736   2.873333   \n...          ...       ...       ...       ...       ...       ...        ...   \n441908  3.346856  4.300481  4.226472  3.316025  6.942172  6.957008  10.023862   \n441675  3.344460  4.513820  4.422115  5.289645  6.456045  8.784964   2.803146   \n441765  3.346856  5.901843  4.917949  3.316025  5.491019  7.469235   2.824619   \n305133  3.346433  4.590048  5.432556  3.231599  5.078104  8.873241   2.873333   \n305238  3.346433  5.479849  6.577619  3.231599  4.502487  8.040511   2.873333   \n\n            EDC3   CACNA1H      GLDC  ...        24        25        26  \\\n249540  4.301549  6.221798  1.367038  ...  0.074042 -0.087038 -0.024521   \n441885  4.018516  3.205853  2.990778  ...  0.054780 -0.014115 -0.034533   \n441672  4.717642  3.252228  2.992718  ...  0.048165 -0.064514  0.199639   \n441746  4.180924  3.205853  2.990778  ...  0.027432 -0.070788 -0.019618   \n305151  4.054779  3.192964  2.993512  ...  0.007469  0.030973 -0.141391   \n...          ...       ...       ...  ...       ...       ...       ...   \n441908  4.122144  3.205853  3.008633  ...  0.082466  0.110974 -0.013309   \n441675  5.828296  3.252228  2.992718  ... -0.045137 -0.053588  0.000039   \n441765  4.018516  3.205853  2.990778  ...  0.005693 -0.001616  0.034910   \n305133  4.025407  6.156487  3.199910  ...  0.142164 -0.008983  0.042038   \n305238  4.025407  3.192964  3.051916  ...  0.027825 -0.099578  0.008022   \n\n              27        28        29        30        31        32        33  \n249540  0.020580 -0.072276 -0.007125 -0.035043 -0.009919 -0.014100  0.041364  \n441885 -0.010267  0.011849 -0.051400 -0.041256 -0.025779 -0.009573  0.016152  \n441672  0.125585  0.004960  0.118577 -0.068890 -0.089174  0.031909 -0.056347  \n441746 -0.095424 -0.016514 -0.026200 -0.008108  0.077148  0.039271  0.066759  \n305151  0.014249  0.004854 -0.127573 -0.009636 -0.044794  0.007050  0.070754  \n...          ...       ...       ...       ...       ...       ...       ...  \n441908  0.005476  0.104104  0.035150  0.013193 -0.026528  0.036514  0.027907  \n441675  0.005054 -0.013336 -0.004936 -0.023907  0.034399  0.074089 -0.091005  \n441765 -0.020828 -0.053719 -0.005217  0.009449 -0.003725  0.014639 -0.049633  \n305133 -0.006486  0.002535  0.012338  0.039600 -0.003016 -0.003724 -0.077369  \n305238 -0.085867 -0.010367 -0.025849 -0.020694  0.012183  0.054392  0.087240  \n\n[193 rows x 134 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MYH7B</th>\n      <th>ADRA2A</th>\n      <th>GRB7</th>\n      <th>LRP12</th>\n      <th>MSRA</th>\n      <th>MRPL17</th>\n      <th>HSPB3</th>\n      <th>EDC3</th>\n      <th>CACNA1H</th>\n      <th>GLDC</th>\n      <th>...</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>249540</th>\n      <td>4.081220</td>\n      <td>5.513807</td>\n      <td>4.859041</td>\n      <td>5.341824</td>\n      <td>4.396074</td>\n      <td>9.662919</td>\n      <td>2.676551</td>\n      <td>4.301549</td>\n      <td>6.221798</td>\n      <td>1.367038</td>\n      <td>...</td>\n      <td>0.074042</td>\n      <td>-0.087038</td>\n      <td>-0.024521</td>\n      <td>0.020580</td>\n      <td>-0.072276</td>\n      <td>-0.007125</td>\n      <td>-0.035043</td>\n      <td>-0.009919</td>\n      <td>-0.014100</td>\n      <td>0.041364</td>\n    </tr>\n    <tr>\n      <th>441885</th>\n      <td>3.346856</td>\n      <td>4.321042</td>\n      <td>4.915306</td>\n      <td>3.316025</td>\n      <td>5.162939</td>\n      <td>6.609321</td>\n      <td>2.824619</td>\n      <td>4.018516</td>\n      <td>3.205853</td>\n      <td>2.990778</td>\n      <td>...</td>\n      <td>0.054780</td>\n      <td>-0.014115</td>\n      <td>-0.034533</td>\n      <td>-0.010267</td>\n      <td>0.011849</td>\n      <td>-0.051400</td>\n      <td>-0.041256</td>\n      <td>-0.025779</td>\n      <td>-0.009573</td>\n      <td>0.016152</td>\n    </tr>\n    <tr>\n      <th>441672</th>\n      <td>3.344460</td>\n      <td>4.884633</td>\n      <td>9.315200</td>\n      <td>3.228507</td>\n      <td>4.330381</td>\n      <td>9.679237</td>\n      <td>2.803146</td>\n      <td>4.717642</td>\n      <td>3.252228</td>\n      <td>2.992718</td>\n      <td>...</td>\n      <td>0.048165</td>\n      <td>-0.064514</td>\n      <td>0.199639</td>\n      <td>0.125585</td>\n      <td>0.004960</td>\n      <td>0.118577</td>\n      <td>-0.068890</td>\n      <td>-0.089174</td>\n      <td>0.031909</td>\n      <td>-0.056347</td>\n    </tr>\n    <tr>\n      <th>441746</th>\n      <td>3.346856</td>\n      <td>5.651840</td>\n      <td>4.915306</td>\n      <td>3.316025</td>\n      <td>4.950791</td>\n      <td>8.296965</td>\n      <td>2.824619</td>\n      <td>4.180924</td>\n      <td>3.205853</td>\n      <td>2.990778</td>\n      <td>...</td>\n      <td>0.027432</td>\n      <td>-0.070788</td>\n      <td>-0.019618</td>\n      <td>-0.095424</td>\n      <td>-0.016514</td>\n      <td>-0.026200</td>\n      <td>-0.008108</td>\n      <td>0.077148</td>\n      <td>0.039271</td>\n      <td>0.066759</td>\n    </tr>\n    <tr>\n      <th>305151</th>\n      <td>3.346433</td>\n      <td>6.204796</td>\n      <td>4.894265</td>\n      <td>3.231599</td>\n      <td>5.866692</td>\n      <td>8.514736</td>\n      <td>2.873333</td>\n      <td>4.054779</td>\n      <td>3.192964</td>\n      <td>2.993512</td>\n      <td>...</td>\n      <td>0.007469</td>\n      <td>0.030973</td>\n      <td>-0.141391</td>\n      <td>0.014249</td>\n      <td>0.004854</td>\n      <td>-0.127573</td>\n      <td>-0.009636</td>\n      <td>-0.044794</td>\n      <td>0.007050</td>\n      <td>0.070754</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>441908</th>\n      <td>3.346856</td>\n      <td>4.300481</td>\n      <td>4.226472</td>\n      <td>3.316025</td>\n      <td>6.942172</td>\n      <td>6.957008</td>\n      <td>10.023862</td>\n      <td>4.122144</td>\n      <td>3.205853</td>\n      <td>3.008633</td>\n      <td>...</td>\n      <td>0.082466</td>\n      <td>0.110974</td>\n      <td>-0.013309</td>\n      <td>0.005476</td>\n      <td>0.104104</td>\n      <td>0.035150</td>\n      <td>0.013193</td>\n      <td>-0.026528</td>\n      <td>0.036514</td>\n      <td>0.027907</td>\n    </tr>\n    <tr>\n      <th>441675</th>\n      <td>3.344460</td>\n      <td>4.513820</td>\n      <td>4.422115</td>\n      <td>5.289645</td>\n      <td>6.456045</td>\n      <td>8.784964</td>\n      <td>2.803146</td>\n      <td>5.828296</td>\n      <td>3.252228</td>\n      <td>2.992718</td>\n      <td>...</td>\n      <td>-0.045137</td>\n      <td>-0.053588</td>\n      <td>0.000039</td>\n      <td>0.005054</td>\n      <td>-0.013336</td>\n      <td>-0.004936</td>\n      <td>-0.023907</td>\n      <td>0.034399</td>\n      <td>0.074089</td>\n      <td>-0.091005</td>\n    </tr>\n    <tr>\n      <th>441765</th>\n      <td>3.346856</td>\n      <td>5.901843</td>\n      <td>4.917949</td>\n      <td>3.316025</td>\n      <td>5.491019</td>\n      <td>7.469235</td>\n      <td>2.824619</td>\n      <td>4.018516</td>\n      <td>3.205853</td>\n      <td>2.990778</td>\n      <td>...</td>\n      <td>0.005693</td>\n      <td>-0.001616</td>\n      <td>0.034910</td>\n      <td>-0.020828</td>\n      <td>-0.053719</td>\n      <td>-0.005217</td>\n      <td>0.009449</td>\n      <td>-0.003725</td>\n      <td>0.014639</td>\n      <td>-0.049633</td>\n    </tr>\n    <tr>\n      <th>305133</th>\n      <td>3.346433</td>\n      <td>4.590048</td>\n      <td>5.432556</td>\n      <td>3.231599</td>\n      <td>5.078104</td>\n      <td>8.873241</td>\n      <td>2.873333</td>\n      <td>4.025407</td>\n      <td>6.156487</td>\n      <td>3.199910</td>\n      <td>...</td>\n      <td>0.142164</td>\n      <td>-0.008983</td>\n      <td>0.042038</td>\n      <td>-0.006486</td>\n      <td>0.002535</td>\n      <td>0.012338</td>\n      <td>0.039600</td>\n      <td>-0.003016</td>\n      <td>-0.003724</td>\n      <td>-0.077369</td>\n    </tr>\n    <tr>\n      <th>305238</th>\n      <td>3.346433</td>\n      <td>5.479849</td>\n      <td>6.577619</td>\n      <td>3.231599</td>\n      <td>4.502487</td>\n      <td>8.040511</td>\n      <td>2.873333</td>\n      <td>4.025407</td>\n      <td>3.192964</td>\n      <td>3.051916</td>\n      <td>...</td>\n      <td>0.027825</td>\n      <td>-0.099578</td>\n      <td>0.008022</td>\n      <td>-0.085867</td>\n      <td>-0.010367</td>\n      <td>-0.025849</td>\n      <td>-0.020694</td>\n      <td>0.012183</td>\n      <td>0.054392</td>\n      <td>0.087240</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows × 134 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_cols = X_train_emb_abs_34_dim.columns.to_list()\n",
    "X_ge_100_train, X_ge_100_test = X_ge_train[feats_100], X_ge_test[feats_100]\n",
    "\n",
    "X_raw_emb_train = pd.merge(X_ge_100_train, X_train_emb_abs_34_dim, left_index=True, right_index=True)\n",
    "X_raw_emb_test = pd.merge(X_ge_100_test, X_test_emb_abs_34_dim, left_index=True, right_index=True)\n",
    "X_raw_emb_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "raw_pipe = make_pipeline(ColumnSelector(cols=feats_100), clf_raw_acc)\n",
    "emb_pip = make_pipeline(ColumnSelector(cols=emb_cols), clf_emb_34_acc)\n",
    "sclf_acc_v1 = StackingClassifier(classifiers=[raw_pipe, emb_pip], meta_classifier=LogisticRegression())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "balanced_accuracy      0.748700\nrecall_0               0.625000\nprecision_0            0.642282\nrecall_1               0.872401\nprecision_1            0.865407\nauc                    0.824114\nspecificity            0.625000\naverage_precision_0    0.219227\ndtype: float64"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_sclf_1 = cross_validate(sclf_acc_v1, X_raw_emb_train, y_train_emb_abs_df,\n",
    "                                   n_jobs=-1, scoring=scoring, cv=st_cv)\n",
    "\n",
    "cv_results_sclf_1_df = get_scores(cv_results_sclf_1)\n",
    "cv_results_sclf_1_df.mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:40:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": "balanced_accuracy      0.631939\nrecall_0               0.411765\nprecision_0            0.500000\nrecall_1               0.852113\nprecision_1            0.801325\nauc                    0.711958\nspecificity            0.411765\naverage_precision_0    0.225676\ndtype: float64"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf_acc_v1.fit(X_raw_emb_train, y_ge_train)\n",
    "test_scores_ens = calc_scores(sclf_acc_v1, X_raw_emb_test, y_ge_test)\n",
    "test_scores_ens.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Balanced Opt\n",
      "\n",
      "\t\t\tRaw\t\t\t\tEnsemble\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t74.53%\t\t\t\t74.87%\n",
      "\n",
      "recall_0:\t\t63.33%\t\t\t\t62.50%\n",
      "\n",
      "precision_0:\t\t62.14%\t\t\t\t64.23%\n",
      "\n",
      "recall_1:\t\t85.72%\t\t\t\t87.24%\n",
      "\n",
      "precision_1:\t\t86.62%\t\t\t\t86.54%\n",
      "\n",
      "auc:\t\t\t84.56%\t\t\t\t82.41%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Balanced Opt\n",
      "\n",
      "\t\t\tRaw\t\t\t\tEnsemble\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t64.37%\t\t\t\t63.19%\n",
      "\n",
      "recall_0:\t\t47.06%\t\t\t\t41.18%\n",
      "\n",
      "precision_0:\t\t48.00%\t\t\t\t50.00%\n",
      "\n",
      "recall_1:\t\t81.69%\t\t\t\t85.21%\n",
      "\n",
      "precision_1:\t\t81.12%\t\t\t\t80.13%\n",
      "\n",
      "auc:\t\t\t72.73%\t\t\t\t71.20%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare stacked ensemble with raw base model\n",
    "print_score_comparison_v2(cv_score_raw_acc, cv_results_sclf_1_df, test_scores_raw_acc, test_scores_ens, header_1=\"Raw\", header_2=\"Ensemble\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Balanced Opt\n",
      "\n",
      "\t\t\tEmb(34 dim)\t\t\t\tEnsemble\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t78.06%\t\t\t\t74.87%\n",
      "\n",
      "recall_0:\t\t88.33%\t\t\t\t62.50%\n",
      "\n",
      "precision_0:\t\t50.24%\t\t\t\t64.23%\n",
      "\n",
      "recall_1:\t\t67.78%\t\t\t\t87.24%\n",
      "\n",
      "precision_1:\t\t94.31%\t\t\t\t86.54%\n",
      "\n",
      "auc:\t\t\t85.71%\t\t\t\t82.41%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Balanced Opt\n",
      "\n",
      "\t\t\tEmb(34 dim)\t\t\t\tEnsemble\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t66.48%\t\t\t\t63.19%\n",
      "\n",
      "recall_0:\t\t74.51%\t\t\t\t41.18%\n",
      "\n",
      "precision_0:\t\t39.18%\t\t\t\t50.00%\n",
      "\n",
      "recall_1:\t\t58.45%\t\t\t\t85.21%\n",
      "\n",
      "precision_1:\t\t86.46%\t\t\t\t80.13%\n",
      "\n",
      "auc:\t\t\t71.06%\t\t\t\t71.20%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare stacked ensemble with embedding base model\n",
    "print_score_comparison_v2(cv_score_emb_34_acc, cv_results_sclf_1_df, test_scores_emb_34_acc, test_scores_ens, header_1=\"Emb(34 dim)\", header_2=\"Ensemble\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[12:43:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:43:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 4.51 seconds.\n",
      "Best Score: 79.293%\n",
      "{'subsample': 1.0, 'scale_pos_weight': 0.3, 'n_estimators': 50, 'min_child_weight': 5, 'max_depth': 3, 'max_delta_step': 4, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.6}\n",
      "[12:44:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "====================================================\n",
      "CV Score: \n",
      "balanced_accuracy      0.792925\n",
      "recall_0               0.783333\n",
      "precision_0            0.592256\n",
      "recall_1               0.802517\n",
      "precision_1            0.910404\n",
      "auc                    0.854757\n",
      "specificity            0.783333\n",
      "average_precision_0    0.229468\n",
      "dtype: float64\n",
      "\n",
      "====================================================\n",
      "Test Score:\n",
      "balanced_accuracy      0.658313\n",
      "recall_0               0.549020\n",
      "precision_0            0.459016\n",
      "recall_1               0.767606\n",
      "precision_1            0.825758\n",
      "auc                    0.729771\n",
      "specificity            0.549020\n",
      "average_precision_0    0.223658\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    4.4s finished\n"
     ]
    }
   ],
   "source": [
    "params_raw_emb_acc, clf_raw_emb_acc, cv_score_raw_emb_acc, test_scores_raw_emb_acc = evaluate_ge((X_raw_emb_train, X_raw_emb_test, y_ge_train, y_ge_test), split=False, rand_scoring=\"balanced_accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Balanced Opt\n",
      "\n",
      "\t\t\tRaw\t\t\t\tRaw + Emb\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t74.53%\t\t\t\t79.29%\n",
      "\n",
      "recall_0:\t\t63.33%\t\t\t\t78.33%\n",
      "\n",
      "precision_0:\t\t62.14%\t\t\t\t59.23%\n",
      "\n",
      "recall_1:\t\t85.72%\t\t\t\t80.25%\n",
      "\n",
      "precision_1:\t\t86.62%\t\t\t\t91.04%\n",
      "\n",
      "auc:\t\t\t84.56%\t\t\t\t85.48%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Balanced Opt\n",
      "\n",
      "\t\t\tRaw\t\t\t\tRaw + Emb\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t64.37%\t\t\t\t65.83%\n",
      "\n",
      "recall_0:\t\t47.06%\t\t\t\t54.90%\n",
      "\n",
      "precision_0:\t\t48.00%\t\t\t\t45.90%\n",
      "\n",
      "recall_1:\t\t81.69%\t\t\t\t76.76%\n",
      "\n",
      "precision_1:\t\t81.12%\t\t\t\t82.58%\n",
      "\n",
      "auc:\t\t\t72.73%\t\t\t\t72.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare raw with raw+emb model\n",
    "print_score_comparison_v2(cv_score_raw_acc, cv_score_raw_emb_acc, test_scores_raw_acc, test_scores_raw_emb_acc, header_1=\"Raw\", header_2=\"Raw + Emb\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation set - Balanced Opt\n",
      "\n",
      "\t\t\tEmb(34 dim)\t\t\t\tRaw + Emb\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t78.06%\t\t\t\t79.29%\n",
      "\n",
      "recall_0:\t\t88.33%\t\t\t\t78.33%\n",
      "\n",
      "precision_0:\t\t50.24%\t\t\t\t59.23%\n",
      "\n",
      "recall_1:\t\t67.78%\t\t\t\t80.25%\n",
      "\n",
      "precision_1:\t\t94.31%\t\t\t\t91.04%\n",
      "\n",
      "auc:\t\t\t85.71%\t\t\t\t85.48%\n",
      "\n",
      "\n",
      "\n",
      "\tTest set - Balanced Opt\n",
      "\n",
      "\t\t\tEmb(34 dim)\t\t\t\tRaw + Emb\n",
      "\t\t-------------------------------------------------------\n",
      "balanced_accuracy:\t66.48%\t\t\t\t65.83%\n",
      "\n",
      "recall_0:\t\t74.51%\t\t\t\t54.90%\n",
      "\n",
      "precision_0:\t\t39.18%\t\t\t\t45.90%\n",
      "\n",
      "recall_1:\t\t58.45%\t\t\t\t76.76%\n",
      "\n",
      "precision_1:\t\t86.46%\t\t\t\t82.58%\n",
      "\n",
      "auc:\t\t\t71.06%\t\t\t\t72.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare emb with raw + emb model\n",
    "print_score_comparison_v2(cv_score_emb_34_acc, cv_score_raw_emb_acc, test_scores_emb_34_acc, test_scores_raw_emb_acc, header_1=\"Emb(34 dim)\", header_2=\"Raw + Emb\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}